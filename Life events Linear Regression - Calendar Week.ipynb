{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 996,
   "id": "f4edb049",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (0.12.2)\n",
      "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (0.5.1)\n",
      "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->statsmodels) (2021.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5->statsmodels) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: vaderSentiment in /home/sagarwal420/.local/lib/python3.6/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vaderSentiment) (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->vaderSentiment) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->vaderSentiment) (2018.1.18)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->vaderSentiment) (1.22)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (6.0.7)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.8.1)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.5.4)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.3.3)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.1.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (3.0.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.8.4)\n",
      "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (5.1.3)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.10.0)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.5.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert) (2.0.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in /usr/local/lib/python3.6/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (7.0.3)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.6/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (1.5.1)\n",
      "Requirement already satisfied: async-generator in /usr/local/lib/python3.6/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (1.10)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (22.3.0)\n",
      "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (6.1)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert) (3.2.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (4.8.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (1.15.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/lib/python3/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (17.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (58.1.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert) (5.1.0)\n",
      "Requirement already satisfied: webencodings in /usr/lib/python3/dist-packages (from bleach->nbconvert) (0.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (21.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert) (2.4.7)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tabulate in /home/sagarwal420/.local/lib/python3.6/site-packages (0.8.10)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (6.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (1.0.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (756 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 756 kB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from tqdm->nltk) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from importlib-resources->tqdm->nltk) (3.5.0)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.6.7 regex-2022.10.31 tqdm-4.64.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install statsmodels\n",
    "!pip3 install vaderSentiment\n",
    "!pip3 install nbconvert\n",
    "!pip3 install tabulate\n",
    "!pip3 install --upgrade scipy\n",
    "!pip3 install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4ee099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "from statistics import mean\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.gof import chisquare as chisquare\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "import csv\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "b39d1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.datetime.strptime(\"2020-08-22\", \"%Y-%m-%d\").date()\n",
    "\n",
    "def update_end_date(row, columnName, latestDate):\n",
    "    if row[columnName] == row[columnName]:\n",
    "        return row[columnName]\n",
    "    else:\n",
    "        return latestDate\n",
    "\n",
    "def getDays(row, beginColumnName, endColumnName):\n",
    "    return (row[endColumnName] - row[beginColumnName]).days\n",
    "\n",
    "def calculate_recency(row, columnName):\n",
    "    return (current_date - row[columnName]).days\n",
    "\n",
    "def lookup_index(row, columnName, array):\n",
    "    if(row[columnName] not in array):\n",
    "        return -1\n",
    "    return array.index(row[columnName]) + 1\n",
    "\n",
    "def fix_signficance(row):\n",
    "    if('significance' in row['valence']):\n",
    "        return row['valence']\n",
    "    else:\n",
    "        return row['significance']\n",
    "\n",
    "def fix_valence(row):\n",
    "    if('significance' in row['significance']):\n",
    "        return row['valence']\n",
    "    else:\n",
    "        return row['significance']  \n",
    "\n",
    "def compute_sentiment(row):\n",
    "    post = row['Text']\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(post)\n",
    "    sentiment = 0\n",
    "    if (vs[\"neu\"]>0.8):\n",
    "        sentiment = 0\n",
    "    elif (vs[\"pos\"]==vs[\"neg\"]):\n",
    "        sentiment = 0\n",
    "    elif (vs[\"pos\"]>vs[\"neg\"]):\n",
    "        sentiment = 1\n",
    "    elif (vs[\"neg\"]>vs[\"neu\"]):\n",
    "        sentiment = -1\n",
    "    return sentiment\n",
    "\n",
    "def convert_valence_to_sentiment(row):\n",
    "    valence = row['valence']\n",
    "    retVal = 0\n",
    "    if (valence == 'Neither Positive or Negative'):\n",
    "        retVal = 0\n",
    "    elif(\"Positive\" in valence):\n",
    "        retVal = 1\n",
    "    elif(\"Negative\" in valence):\n",
    "        retVal = -1\n",
    "    return retVal\n",
    "\n",
    "def update_status(row, columnName):\n",
    "    if row[columnName] == row[columnName]:\n",
    "        if \"ongoing\" in row[columnName].lower():\n",
    "            return \"Ongoing\"\n",
    "        return row[columnName]\n",
    "    else:\n",
    "        return \"Ended\"\n",
    "\n",
    "def update_education_level(row, columnName):\n",
    "    if 'college' in row[columnName].lower():\n",
    "        return \"College\"\n",
    "    elif 'doctoral' in row[columnName].lower():\n",
    "        return \"Doctoral\"\n",
    "    elif 'master' in row[columnName].lower() or 'grad' in row[columnName].lower():\n",
    "        return \"Graduate\"\n",
    "    elif 'hs' in row[columnName].lower() or 'high school' in row[columnName].lower():\n",
    "        return \"High\"\n",
    "    \n",
    "    return row[columnName]\n",
    "\n",
    "def normalize(series):\n",
    "    return (series - series.mean())/series.std()\n",
    "\n",
    "def star_pvals(p_val):\n",
    "    if p_val<0.001:\n",
    "        return \"***\"\n",
    "    elif p_val<0.01:\n",
    "        return \"**\"\n",
    "    elif p_val<0.05:\n",
    "        return \"*\"\n",
    "    return \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4331d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_variables             = 'shipley_vocab + shipley_abs + openness + conscientiousness + extraversion + agreeableness + neuroticism + pos_affect + neg_affect + stai_trait + education_level + psqi + age + gender'\n",
    "combined_control_variables    = 'shipley_vocab + shipley_abs + openness + conscientiousness + extraversion + agreeableness + neuroticism + pos_affect + neg_affect + stai_trait + education_level + psqi + age + gender + data_type'\n",
    "\n",
    "sr_life_event_variables       = 'Anticipation_sr + LifeEventType_sr + valence_sr + recency_sr + status_sr + Intimacy_sr + Scope_sr + significance_sr'\n",
    "sm_life_event_variables       = 'Anticipation_sm + LifeEventType_sm + valence_sm + recency_sm + status_sm + Intimacy_sm + Scope_sm + significance_sm'\n",
    "combined_life_event_variables = 'Anticipation + LifeEventFamily + valence + recency + status + Intimacy + Scope + data_type'\n",
    "\n",
    "sr_all_variables     = 'shipley_vocab + shipley_abs + openness + conscientiousness + extraversion + agreeableness + neuroticism + pos_affect + neg_affect + stai_trait + education_level + psqi + age + gender + Anticipation + LifeEventFamily + valence + recency + status + Intimacy + Scope + significance_label'\n",
    "sm_all_variables     = 'shipley_vocab + shipley_abs + openness + conscientiousness + extraversion + agreeableness + neuroticism + pos_affect + neg_affect + stai_trait + education_level + psqi + age + gender + Anticipation + LifeEventFamily + valence + recency + status + Intimacy + Scope'\n",
    "combined_all_variables        = 'shipley_vocab + shipley_abs + openness + conscientiousness + extraversion + agreeableness + neuroticism + pos_affect + neg_affect + stai_trait + education_level + psqi + age + gender + Anticipation + LifeEventFamily + valence + recency + status + Intimacy + Scope + data_type'\n",
    "# combined_sm_sr_all_variables  = 'shipley_vocab_sm + shipley_vocab_sr + shipley_abs_sm + shipley_abs_sr + openness_sm + openness_sr + conscientiousness_sm + conscientiousness_sr + extraversion_sm + extraversion_sr + agreeableness_sm + agreeableness_sr + neuroticism_sm + neuroticism_sr + pos_affect_sm + pos_affect_sr + neg_affect_sm + neg_affect_sr + stai_trait_sm + stai_trait_sr + education_level_sm + education_level_sr + psqi_sm + psqi_sr + age_sm + age_sr + gender_sm + gender_sr + Anticipation_sm + Anticipation_sr + LifeEventFamily_sm + LifeEventFamily_sr + valence_sm + valence_sr + Intimacy_sm + Intimacy_sr + Scope_sm + Scope_sr + data_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "bcc9ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demographics_data():\n",
    "    demographics_data = pd.read_csv('data/igtbs_demographics_complete.csv', parse_dates=True)\n",
    "    demographics_data = demographics_data[['age','gender','snapshot_id', 'shipley.vocab', 'shipley.abs', 'openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism','pos.affect','neg.affect','stai.trait','psqi','educ']]\n",
    "    demographics_data['education_level'] = demographics_data.apply(update_education_level, columnName='educ', axis=1)\n",
    "    demographics_data = demographics_data.drop(columns=['educ'])\n",
    "    demographics_data = demographics_data.rename(columns={\n",
    "        'shipley.vocab': 'shipley_vocab',\n",
    "        'shipley.abs': 'shipley_abs',\n",
    "        'pos.affect': 'pos_affect',\n",
    "        'neg.affect': 'neg_affect',\n",
    "        'stai.trait': 'stai_trait'\n",
    "    })\n",
    "    demographics_data = demographics_data.dropna()\n",
    "    return demographics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "d8ae65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_survey_categories():\n",
    "    df_self_reported_categories = pd.read_csv('data/Life Events Categories Mapping - Self-Reported Categories.csv')\n",
    "    return df_self_reported_categories\n",
    "\n",
    "def load_survey_data_without_categories():\n",
    "    df_survey = pd.read_csv('data/Superimposed/LifeEvents_Curated_non_blinded.csv', parse_dates=True)    \n",
    "    df_survey = df_survey[['snapshot_id', 'description','UpdatedBeginDate', 'UpdatedEndDate','life_event_type', 'work_perf_impact', 'significance','valence', 'ended_or_ongoing']]\n",
    "\n",
    "    # Date manipulation\n",
    "    latest_date = max(datetime.datetime.strptime(str(x), \"%Y-%m-%d\").date() if x == x else datetime.date.min for x in df_survey['UpdatedEndDate'])\n",
    "    latest_date = max(latest_date, max(datetime.datetime.strptime(str(x), \"%Y-%m-%d\").date() if x == x else datetime.date.min for x in df_survey['UpdatedBeginDate']))\n",
    "    df_survey = df_survey.drop(df_survey[df_survey['UpdatedBeginDate'].isnull() == True].index)\n",
    "\n",
    "    df_survey['UpdatedBeginDate'] = pd.to_datetime(df_survey['UpdatedBeginDate'], format = '%Y-%m-%d').dt.date\n",
    "    df_survey['UpdatedEndDate'] = df_survey.apply(update_end_date, columnName='UpdatedEndDate', latestDate=latest_date, axis=1)\n",
    "    df_survey['UpdatedEndDate'] = pd.to_datetime(df_survey['UpdatedEndDate'], format = '%Y-%m-%d').dt.date\n",
    "\n",
    "    df_survey['num_of_days'] = df_survey.apply(getDays, endColumnName='UpdatedEndDate', beginColumnName='UpdatedBeginDate', axis=1)\n",
    "    df_survey['recency'] = df_survey.apply(calculate_recency, columnName='UpdatedEndDate', axis=1)\n",
    "\n",
    "    # Update values for valence and significance\n",
    "    df_survey.replace({'valence': {np.nan: 'Neither Positive or Negative'}, 'significance': {np.nan: 'Neither Positive or Negative'}}, inplace=True)\n",
    "    df_survey['fixed_signficance'] = df_survey.apply(fix_signficance, axis = 1)\n",
    "    df_survey['fixed_valence'] = df_survey.apply(fix_valence, axis = 1)\n",
    "    df_survey = df_survey.drop(columns = ['valence', 'significance'])\n",
    "    df_survey = df_survey.rename(columns={\"fixed_signficance\": \"significance\", \"fixed_valence\": \"valence\"})\n",
    "    df_survey['valence'] = df_survey.apply(convert_valence_to_sentiment, axis=1)\n",
    "    df_survey['ended_or_ongoing'] = df_survey.apply(update_status, columnName='ended_or_ongoing', axis=1)\n",
    "\n",
    "    # Select columns we are interested in\n",
    "    df_survey = df_survey[['snapshot_id', 'description', 'UpdatedBeginDate', 'UpdatedEndDate', 'significance', 'valence', 'ended_or_ongoing', 'recency']]\n",
    "\n",
    "    # Label encoding for significance\n",
    "    le_significance = LabelEncoder()\n",
    "    le_significance.fit(df_survey['significance'].values)\n",
    "    df_survey['significance_label'] = df_survey.apply(lambda x: le_significance.transform([x['significance']])[0], axis=1)\n",
    "    df_survey = df_survey.drop(columns=['significance'])\n",
    "\n",
    "    return df_survey\n",
    "\n",
    "def load_survey_data():\n",
    "    df_survey_without_categories = load_survey_data_without_categories()\n",
    "    df_self_reported_categories = load_survey_categories()\n",
    "    df_survey = pd.merge(df_survey_without_categories, df_self_reported_categories, how=\"inner\", left_on=\"description\", right_on=\"SR_LifeEvent\")\n",
    "    df_survey = df_survey.drop(columns=['description', 'SR_LifeEvent', 'LifeEventFinal', 'LifeEventFamily2'])\n",
    "    df_survey['significance_label'] = (df_survey['significance_label'] - df_survey['significance_label'].min()) / (df_survey['significance_label'].max() - df_survey['significance_label'].min())    \n",
    "    df_survey = df_survey.rename(columns={'ended_or_ongoing':'status', 'LifeEventFamily': 'LifeEventType', 'significance_label': 'significance'})\n",
    "\n",
    "\n",
    "    return df_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "480efef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_social_media_categories():\n",
    "    df_social_media_categories = pd.read_csv('data/Life Events Categories Mapping - Social Media Categories-2.csv')\n",
    "    df_social_media_categories['SignificanceRank'] = df_social_media_categories['SignificanceRank'].fillna(df_social_media_categories['SignificanceRank'].max())\n",
    "    return df_social_media_categories\n",
    "\n",
    "def load_social_media_data_without_categories():\n",
    "    df_social_media_data = pd.read_csv('data/Superimposed/Facebook Data For Life Events-Combined - FB Data.csv')\n",
    "    df_social_media_data = df_social_media_data[['snapshot_id', 'created_time', 'Text', 'final_life_event_category_2', 'ended/ongoing']]\n",
    "    df_social_media_data = df_social_media_data.replace({'PostiveMove':'PositiveMove', 'Negative Move':'NegativeMove'})\n",
    "    df_social_media_data = df_social_media_data.drop(df_social_media_data[((df_social_media_data['final_life_event_category_2'].isnull() == True))].index)\n",
    "    df_social_media_data['created_date'] = pd.to_datetime(df_social_media_data['created_time'], format = '%Y-%m-%d %H:%M:%S').dt.date\n",
    "    df_social_media_data['valence'] = df_social_media_data.apply(compute_sentiment, axis=1)\n",
    "    df_social_media_data = df_social_media_data.drop(columns=['created_time','Text'])\n",
    "    df_social_media_data['recency'] = df_social_media_data.apply(calculate_recency, columnName='created_date', axis=1)\n",
    "    df_social_media_data['ended/ongoing'] = df_social_media_data.apply(update_status, columnName='ended/ongoing', axis=1)\n",
    "    return df_social_media_data\n",
    "\n",
    "def load_social_media_data():\n",
    "    df_social_media_data = load_social_media_data_without_categories()\n",
    "    df_social_media_categories = load_social_media_categories()\n",
    "    df_social_media_data_with_categories = pd.merge(df_social_media_data, df_social_media_categories, how=\"inner\", left_on='final_life_event_category_2', right_on='SM_LifeEvent')\n",
    "    df_social_media_data_with_categories = df_social_media_data_with_categories.drop(columns=['final_life_event_category_2','SM_LifeEvent','LifeEventFamily2','Comments'])\n",
    "    df_social_media_data_with_categories = df_social_media_data_with_categories.rename(columns={'ended/ongoing':'status', 'LifeEventFamily': 'LifeEventType', 'SignificanceRank': 'significance'})\n",
    "    df_social_media_data_with_categories['significance'].fillna(df_social_media_data_with_categories['significance'].max())\n",
    "    df_social_media_data_with_categories['significance'] = (df_social_media_data_with_categories['significance'] - df_social_media_data_with_categories['significance'].min()) / (df_social_media_data_with_categories['significance'].max() - df_social_media_data_with_categories['significance'].min())    \n",
    "    \n",
    "    return df_social_media_data_with_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "id": "31e5b580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 1097,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_survey_data_without_categories()['significance_label'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6108bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dailies_data():\n",
    "    df_dailies = pd.read_csv('data/Superimposed/dailies_scores.csv', low_memory=False)\n",
    "    df_dailies = df_dailies[['snapshot_id','day', 'alc_status', 'alc.quantity.d', 'anxiety.d', 'pos.affect.d', 'neg.affect.d','sleep.d', 'stress.d']]\n",
    "    df_dailies['day'] = pd.to_datetime(df_dailies['day'], format='%Y-%m-%d').dt.date\n",
    "    df_dailies = df_dailies.rename(columns={'alc.quantity.d': 'alc_quantity',\n",
    "    'anxiety.d': 'anxiety',\n",
    "    'pos.affect.d': 'pos_affect',\n",
    "    'neg.affect.d': 'neg_affect',\n",
    "    'sleep.d': 'sleep',\n",
    "    'stress.d': 'stress'})\n",
    "    df_dailies['sleep'] = df_dailies['sleep'] + 1\n",
    "    return df_dailies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "b667198c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(load_dailies_data()['sleep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3224acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_year(row, columnName):\n",
    "    return row[columnName].isocalendar()[0]\n",
    "\n",
    "def calculate_week(row, columnName):\n",
    "    return row[columnName].isocalendar()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "d115b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stacked_df(dependent_variable):\n",
    "    try:\n",
    "        merged_data = pd.read_csv('Linear Regression/Calendar Week/all_data_stacked_'+dependent_variable+'.csv')\n",
    "    except:\n",
    "        df_dailies = load_dailies_data()\n",
    "        df_social_media = load_social_media_data()\n",
    "        df_survey = load_survey_data()\n",
    "        df_demographics = load_demographics_data()\n",
    "\n",
    "        df_social_media['year'] = df_social_media.apply(calculate_year, columnName='created_date', axis=1)\n",
    "        df_social_media['week'] = df_social_media.apply(calculate_week, columnName='created_date', axis=1)\n",
    "        df_social_media['type'] = 'Social Media'\n",
    "\n",
    "        df_survey['year'] = -1\n",
    "        df_survey['week'] = -1\n",
    "        survey_rows = []\n",
    "        for i, row in df_survey.iterrows():\n",
    "            s_date = row['UpdatedBeginDate']\n",
    "            e_date = row['UpdatedEndDate']\n",
    "            delta = timedelta(days=7)\n",
    "\n",
    "            while s_date <= e_date:\n",
    "                r = row.copy(deep=True)\n",
    "                r['year'] = s_date.isocalendar()[0]\n",
    "                r['week'] = s_date.isocalendar()[1]\n",
    "                r['recency'] = (current_date - s_date).days\n",
    "                survey_rows.append(r.values)\n",
    "                s_date +=delta\n",
    "        df_survey = pd.DataFrame(survey_rows, columns=df_survey.columns)\n",
    "        df_survey['type'] = 'Survey'\n",
    "\n",
    "        df_dailies['year'] = df_dailies.apply(calculate_year, columnName='day', axis=1)\n",
    "        df_dailies['week'] = df_dailies.apply(calculate_week, columnName='day', axis=1)\n",
    "        df_dailies = df_dailies[['snapshot_id', dependent_variable, 'year', 'week']]\n",
    "        df_dailies = df_dailies.groupby(['snapshot_id', 'year', 'week']).mean().reset_index()\n",
    "\n",
    "        merged_data = pd.merge(df_dailies, df_demographics, how=\"inner\", on=\"snapshot_id\")\n",
    "        merged_data_sm = pd.merge(merged_data, df_social_media, how=\"inner\", on=[\"snapshot_id\", \"year\", \"week\"])\n",
    "        merged_data_sr = pd.merge(merged_data, df_survey, how=\"inner\", on=[\"snapshot_id\", \"year\", \"week\"])\n",
    "\n",
    "        merged_data = pd.concat([merged_data_sm,merged_data_sr],ignore_index=True)\n",
    "        \n",
    "        merged_data['recency'] = (merged_data['recency'] - merged_data['recency'].min()) / (merged_data['recency'].max() - merged_data['recency'].min())    \n",
    "        merged_data.to_csv('Linear Regression/Calendar Week/all_data_stacked_'+dependent_variable+'.csv', index=False)\n",
    "        \n",
    "    merged_data = merged_data.drop(columns=['created_date', 'UpdatedBeginDate', 'UpdatedEndDate'])\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "8e9444a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_merged_df(dependent_variable):\n",
    "    try:\n",
    "        merged_data = pd.read_csv('Linear Regression/Calendar Week/all_data_'+dependent_variable+'.csv')\n",
    "    except:\n",
    "        df_dailies = load_dailies_data()\n",
    "        df_social_media = load_social_media_data()\n",
    "        df_survey = load_survey_data()\n",
    "        df_demographics = load_demographics_data()\n",
    "        \n",
    "        df_social_media['year'] = df_social_media.apply(calculate_year, columnName='created_date', axis=1)\n",
    "        df_social_media['week'] = df_social_media.apply(calculate_week, columnName='created_date', axis=1)\n",
    "        df_social_media.rename(columns=lambda x: x+'_sm', inplace=True)\n",
    "        df_social_media = df_social_media.rename(columns={\n",
    "            'snapshot_id_sm': 'snapshot_id',\n",
    "            'week_sm' : 'week',\n",
    "            'year_sm' : 'year'\n",
    "        })\n",
    "\n",
    "        df_survey['year'] = -1\n",
    "        df_survey['week'] = -1\n",
    "        survey_rows = []\n",
    "        for i, row in df_survey.iterrows():\n",
    "            s_date = row['UpdatedBeginDate']\n",
    "            e_date = row['UpdatedEndDate']\n",
    "            delta = timedelta(days=7)\n",
    "\n",
    "            while s_date <= e_date:\n",
    "                r = row.copy(deep=True)\n",
    "                r['year'] = s_date.isocalendar()[0]\n",
    "                r['week'] = s_date.isocalendar()[1]\n",
    "                r['recency'] = (current_date - s_date).days\n",
    "\n",
    "                survey_rows.append(r.values)\n",
    "                s_date +=delta\n",
    "        df_survey = pd.DataFrame(survey_rows, columns=df_survey.columns)\n",
    "        df_survey.rename(columns=lambda x: x+'_sr', inplace=True)\n",
    "        df_survey = df_survey.rename(columns={\n",
    "            'snapshot_id_sr': 'snapshot_id',\n",
    "            'week_sr' : 'week',\n",
    "            'year_sr' : 'year'\n",
    "        })\n",
    "\n",
    "        df_dailies['year'] = df_dailies.apply(calculate_year, columnName='day', axis=1)\n",
    "        df_dailies['week'] = df_dailies.apply(calculate_week, columnName='day', axis=1)\n",
    "        df_dailies = df_dailies[['snapshot_id', dependent_variable, 'year', 'week']]\n",
    "        df_dailies = df_dailies.groupby(['snapshot_id', 'year', 'week']).mean().reset_index()\n",
    "\n",
    "        df_social_media['recency_sm'] = (df_social_media['recency_sm'] - df_social_media['recency_sm'].min()) / (df_social_media['recency_sm'].max() - df_social_media['recency_sm'].min())    \n",
    "        df_survey['recency_sr'] = (df_survey['recency_sr'] - df_survey['recency_sr'].min()) / (df_survey['recency_sr'].max() - df_survey['recency_sr'].min())    \n",
    "        \n",
    "        merged_data = pd.merge(df_dailies, df_demographics, how=\"inner\", on=\"snapshot_id\")\n",
    "        merged_data = pd.merge(merged_data, df_social_media, how=\"left\", on=[\"snapshot_id\", \"year\", \"week\"])\n",
    "        merged_data = pd.merge(merged_data, df_survey, how=\"left\", on=[\"snapshot_id\", \"year\", \"week\"])\n",
    "        \n",
    "\n",
    "        merged_data.to_csv('Linear Regression/Calendar Week/all_data_'+dependent_variable+'.csv', index=False)\n",
    "        \n",
    "    merged_data = merged_data.drop(columns=['created_date_sm', 'UpdatedBeginDate_sr', 'UpdatedEndDate_sr'])\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562e9205",
   "metadata": {},
   "source": [
    "## Run Regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "ee32a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_variables = ['shipley_vocab', 'shipley_abs', 'openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism', 'pos_affect', 'neg_affect', 'stai_trait', 'education_level', 'psqi', 'age', 'gender']\n",
    "sr_life_event_variables = ['Anticipation_sr', 'LifeEventType_sr', 'valence_sr', 'recency_sr', 'status_sr', 'Intimacy_sr', 'Scope_sr', 'significance_sr']\n",
    "sm_life_event_variables = ['Anticipation_sm', 'LifeEventType_sm', 'valence_sm', 'recency_sm', 'status_sm', 'Intimacy_sm', 'Scope_sm', 'significance_sm']\n",
    "life_event_variables = ['Anticipation', 'LifeEventType', 'valence', 'recency', 'status', 'Intimacy', 'Scope', 'significance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "86e26066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(data, formula):\n",
    "    model = smf.ols(formula=formula, data = data).fit()\n",
    "    summary = model.summary2()\n",
    "    a = summary.tables[1]\n",
    "    a['significance'] = a['P>|t|'].apply(lambda x: star_pvals(x))\n",
    "    a.sort_values(by=['P>|t|'], inplace=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "id": "db826ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = build_merged_df('stress')\n",
    "all_data = data[(data['LifeEventType_sr'].notnull()) & (data['LifeEventType_sm'].notnull())]\n",
    "all_data = all_data[all_data['stress'].notnull()]\n",
    "summary = get_summary(all_data, 'stress'+'~'+' + '.join(control_variables))\n",
    "\n",
    "with open('test.txt', 'w') as the_file:\n",
    "    the_file.write(summary.as_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "c889bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(data, formula):\n",
    "    model = smf.ols(formula=formula, data = data).fit()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "b00e89a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_file(data, formula, filename):\n",
    "    t = get_summary(data, formula).as_text()\n",
    "\n",
    "    with open('Linear Regression/Calendar Week/summary/'+filename+'.txt', 'w') as the_file:\n",
    "        the_file.write(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "id": "ca7a3a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_files(dependent_variable):\n",
    "    data = build_merged_df(dependent_variable)\n",
    "    all_data = data[(data['LifeEventType_sr'].notnull()) & (data['LifeEventType_sm'].notnull())]\n",
    "    all_data = all_data[all_data[dependent_variable].notnull()]\n",
    "\n",
    "    generate_summary_file(all_data, dependent_variable+'~'+' + '.join(control_variables), 'all_demo_trait_'+dependent_variable)\n",
    "    generate_summary_file(all_data, dependent_variable+'~'+' + '.join(sm_life_event_variables+sr_life_event_variables), 'all_sm_sr_'+dependent_variable)\n",
    "    generate_summary_file(all_data, dependent_variable+'~'+' + '.join(control_variables+sr_life_event_variables+sm_life_event_variables), 'all_sm_sr_demo_trait_'+dependent_variable)\n",
    "    all_variables = ' + '.join(control_variables+sr_life_event_variables+sm_life_event_variables)\n",
    "    combined_control = ' + '.join(control_variables)\n",
    "    combined_life_event = ' + '.join(sr_life_event_variables+sm_life_event_variables)\n",
    "    final_var = '('+all_variables+')**2 - ' + ' ('+combined_control+')**2 - ' + ' ('+combined_life_event+')**2' + ' + ' + all_variables\n",
    "\n",
    "    generate_summary_file(all_data, ModelDesc.from_formula(dependent_variable+'~'+final_var).describe(), 'all_sm_sr_demo_trait_interaction_'+dependent_variable)\n",
    "\n",
    "    data = build_stacked_df(dependent_variable)\n",
    "    generate_summary_file(data, dependent_variable+'~'+' + '.join(control_variables), 'stacked_demo_trait_'+dependent_variable)\n",
    "    generate_summary_file(data, dependent_variable+'~'+' + '.join(life_event_variables), 'stacked_life_events_'+dependent_variable)\n",
    "    generate_summary_file(data, dependent_variable+'~'+' + '.join(control_variables+life_event_variables + ['type']), 'stacked_demo_trait_life_events_'+dependent_variable)\n",
    "    all_variables = ' + '.join(control_variables + life_event_variables + ['type'])\n",
    "    combined_control = ' + '.join(control_variables+['type'])\n",
    "    combined_life_event = ' + '.join(life_event_variables)\n",
    "    final_var = '('+all_variables+')**2 - ' + ' ('+combined_control+')**2 - ' + ' ('+combined_life_event+')**2' + ' + ' + all_variables\n",
    "    \n",
    "    generate_summary_file(data, ModelDesc.from_formula(dependent_variable+'~'+final_var).describe(), 'stacked_demo_trait_life_events_interaction_'+dependent_variable)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "id": "88ddebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_summary_files('stress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "id": "166465b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_summary_files('sleep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "id": "fec24c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_summary_files('anxiety')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "id": "76dd12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression(data, dependent_variable, variables):\n",
    "\n",
    "    control_data = data[variables['control']+[dependent_variable]].dropna()\n",
    "    train_data, test_data = train_test_split(control_data, test_size=0.20, random_state=805)\n",
    "    f_exp = test_data[dependent_variable].values\n",
    "\n",
    "    baseline_formula = dependent_variable+'~'+' + '.join(variables['control'])\n",
    "    baseline = smf.ols(formula=baseline_formula,\n",
    "                  data = control_data).fit()\n",
    "    baseline_train = smf.ols(formula=baseline_formula,\n",
    "                  data = train_data).fit()\n",
    "    f_obs = baseline_train.predict(test_data).values\n",
    "    pearson_baseline = scipy.stats.pearsonr(f_obs, f_exp)\n",
    "    \n",
    "    \n",
    "    life_events_data = data[variables['life_events']+[dependent_variable]].dropna()    \n",
    "    train_data, test_data = train_test_split(life_events_data, test_size=0.20, random_state=805)\n",
    "    f_exp = test_data[dependent_variable].values\n",
    "\n",
    "    life_events_formula = dependent_variable+'~'+' + '.join(variables['life_events'])\n",
    "    life_events = smf.ols(formula=life_events_formula, data = life_events_data).fit() \n",
    "    life_events_train = smf.ols(formula=life_events_formula, data = train_data).fit()     \n",
    "    f_obs = life_events_train.predict(test_data).values\n",
    "    pearson_life_events = scipy.stats.pearsonr(f_obs, f_exp)    \n",
    "\n",
    "    \n",
    "    all_variable_data = data[variables['all_variables']+[dependent_variable]].dropna()\n",
    "    train_data, test_data = train_test_split(all_variable_data, test_size=0.20, random_state=805)\n",
    "    f_exp = test_data[dependent_variable].values    \n",
    "    \n",
    "    all_variables_formula = dependent_variable+'~'+' + '.join(variables['all_variables'])\n",
    "    all_variables = smf.ols(formula=all_variables_formula, data = all_variable_data).fit() \n",
    "    all_variables_train = smf.ols(formula=all_variables_formula, data = train_data).fit() \n",
    "    f_obs = all_variables_train.predict(test_data).values\n",
    "    pearson_all_variables = scipy.stats.pearsonr(f_obs, f_exp)    \n",
    "\n",
    "    print(\"Baseline R-square adj\", baseline.rsquared_adj)\n",
    "    print(\"Life Events R-square adj\", life_events.rsquared_adj)\n",
    "    print(\"All Variables R-square adj\", all_variables.rsquared_adj)\n",
    "    print()\n",
    "    print(anova_lm(baseline, life_events))\n",
    "    print(anova_lm(baseline, all_variables))\n",
    "    print(anova_lm(life_events, all_variables))\n",
    "    print()\n",
    "    print(\"Baseline pearson\", pearson_baseline)\n",
    "    print(\"Life Events pearson\", pearson_life_events)\n",
    "    print(\"All Variables pearson\", pearson_all_variables)    \n",
    "    return all_variables.rsquared_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "f336ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_regression(dependent_variable):\n",
    "    data = build_merged_df(dependent_variable)\n",
    "\n",
    "    social_media_data = data[data['LifeEventType_sm'].notnull()]\n",
    "    social_media_data = social_media_data[social_media_data[dependent_variable].notnull()]\n",
    "\n",
    "    social_media_variables = {\n",
    "        'control': control_variables,\n",
    "        'life_events': sm_life_event_variables,\n",
    "        'all_variables': sm_life_event_variables + control_variables\n",
    "    }\n",
    "\n",
    "    survey_data = data[data['LifeEventType_sr'].notnull()]\n",
    "    survey_data = survey_data[survey_data[dependent_variable].notnull()]\n",
    "    survey_variables = {\n",
    "        'control': control_variables,\n",
    "        'life_events': sr_life_event_variables,\n",
    "        'all_variables': sr_life_event_variables + control_variables\n",
    "    }\n",
    "    \n",
    "    all_data = data[(data['LifeEventType_sr'].notnull()) & (data['LifeEventType_sm'].notnull())]\n",
    "    all_data = all_data[all_data[dependent_variable].notnull()]\n",
    "    all_data_variables = {\n",
    "        'control': control_variables,\n",
    "        'life_events': sm_life_event_variables + sr_life_event_variables,\n",
    "        'all_variables': sm_life_event_variables + sr_life_event_variables + control_variables\n",
    "    }\n",
    "    \n",
    "    print(\"---------SOCIAL MEDIA---------\")\n",
    "    run_regression(social_media_data, dependent_variable, social_media_variables)\n",
    "    print()\n",
    "    \n",
    "    print(\"---------SURVEY---------\")\n",
    "    run_regression(survey_data, dependent_variable, survey_variables)\n",
    "    print()\n",
    "    \n",
    "    print(\"---------SOCIAL MEDIA + SURVEY---------\")\n",
    "    run_regression(all_data, dependent_variable, all_data_variables)\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "id": "ac51284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stacked_regression(dependent_variable):\n",
    "    data = build_stacked_df(dependent_variable)\n",
    "\n",
    "    all_data = data[data[dependent_variable].notnull()]\n",
    "    all_data_variables = {\n",
    "        'control': control_variables,\n",
    "        'life_events': life_event_variables,\n",
    "        'all_variables': life_event_variables + control_variables + ['type']\n",
    "    }\n",
    "    \n",
    "    print(\"---------SOCIAL MEDIA + SURVEY---------\")\n",
    "    run_regression(all_data, dependent_variable, all_data_variables)\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a64209",
   "metadata": {},
   "source": [
    "### Stress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "id": "d010d51b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------SOCIAL MEDIA---------\n",
      "Baseline R-square adj 0.10907194727705449\n",
      "Life Events R-square adj 0.05937230101079394\n",
      "All Variables R-square adj 0.16120339770359016\n",
      "\n",
      "   df_resid         ssr  df_diff    ss_diff          F  Pr(>F)\n",
      "0     722.0  340.106908      0.0        NaN        NaN     NaN\n",
      "1     726.0  361.068836     -4.0 -20.961928  10.537021     NaN\n",
      "   df_resid         ssr  df_diff    ss_diff         F        Pr(>F)\n",
      "0     722.0  340.106908      0.0        NaN       NaN           NaN\n",
      "1     710.0  314.884029     12.0  25.222879  4.739376  1.742722e-07\n",
      "   df_resid         ssr  df_diff    ss_diff         F        Pr(>F)\n",
      "0     726.0  361.068836      0.0        NaN       NaN           NaN\n",
      "1     710.0  314.884029     16.0  46.184807  6.508589  7.791949e-14\n",
      "\n",
      "Baseline pearson (0.26822579146241304, 0.0009805374505623466)\n",
      "Life Events pearson (0.20493718617723544, 0.012468220369030815)\n",
      "All Variables pearson (0.34620702903789474, 1.6315319295293034e-05)\n",
      "\n",
      "---------SURVEY---------\n",
      "Baseline R-square adj 0.14474634444517942\n",
      "Life Events R-square adj 0.01890986527088767\n",
      "All Variables R-square adj 0.1614982383663217\n",
      "\n",
      "   df_resid         ssr  df_diff    ss_diff          F  Pr(>F)\n",
      "0    1253.0  575.805083      0.0        NaN        NaN     NaN\n",
      "1    1257.0  662.633931     -4.0 -86.828848  41.178038     NaN\n",
      "   df_resid         ssr  df_diff    ss_diff         F    Pr(>F)\n",
      "0    1253.0  575.805083      0.0        NaN       NaN       NaN\n",
      "1    1241.0  559.120280     12.0  16.684803  3.086074  0.000256\n",
      "   df_resid         ssr  df_diff     ss_diff          F        Pr(>F)\n",
      "0    1257.0  662.633931      0.0         NaN        NaN           NaN\n",
      "1    1241.0  559.120280     16.0  103.513651  14.359661  2.992552e-36\n",
      "\n",
      "Baseline pearson (0.3630323716675385, 2.493508159882095e-09)\n",
      "Life Events pearson (0.13209405729357054, 0.03537168384001146)\n",
      "All Variables pearson (0.380227468328361, 3.683257163909643e-10)\n",
      "\n",
      "---------SOCIAL MEDIA + SURVEY---------\n",
      "Baseline R-square adj 0.11325043871416585\n",
      "Life Events R-square adj 0.2968546233183923\n",
      "All Variables R-square adj 0.38336850507768216\n",
      "\n",
      "   df_resid         ssr  df_diff    ss_diff        F        Pr(>F)\n",
      "0     179.0  152.921491      0.0        NaN      NaN           NaN\n",
      "1     172.0  116.516675      7.0  36.404816  7.67717  4.608459e-08\n",
      "   df_resid         ssr  df_diff    ss_diff         F        Pr(>F)\n",
      "0     179.0  152.921491      0.0        NaN       NaN           NaN\n",
      "1     156.0   92.675473     23.0  60.246018  4.409205  9.917020e-09\n",
      "   df_resid         ssr  df_diff    ss_diff         F    Pr(>F)\n",
      "0     172.0  116.516675      0.0        NaN       NaN       NaN\n",
      "1     156.0   92.675473     16.0  23.841202  2.508233  0.001921\n",
      "\n",
      "Baseline pearson (0.41412512313304223, 0.007894400494247042)\n",
      "Life Events pearson (0.5335880790915016, 0.000391750723801014)\n",
      "All Variables pearson (0.5894160673119225, 6.286580616169767e-05)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_all_regression('stress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "id": "c6a39140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------SOCIAL MEDIA + SURVEY---------\n",
      "Baseline R-square adj 0.13179997436452362\n",
      "Life Events R-square adj 0.03450161250160477\n",
      "All Variables R-square adj 0.1620186577882342\n",
      "\n",
      "   df_resid         ssr  df_diff    ss_diff          F  Pr(>F)\n",
      "0    1843.0  785.435389      0.0        NaN        NaN     NaN\n",
      "1    1846.0  874.880195     -3.0 -89.444806  62.909608     NaN\n",
      "   df_resid         ssr  df_diff   ss_diff        F        Pr(>F)\n",
      "0    1843.0  785.435389      0.0       NaN      NaN           NaN\n",
      "1    1829.0  752.338679     14.0  33.09671  5.74721  4.337736e-11\n",
      "   df_resid         ssr  df_diff     ss_diff          F        Pr(>F)\n",
      "0    1846.0  874.880195      0.0         NaN        NaN           NaN\n",
      "1    1829.0  752.338679     17.0  122.541516  17.524056  5.815700e-49\n",
      "\n",
      "Baseline pearson (0.3154235291469514, 4.884945259033782e-10)\n",
      "Life Events pearson (0.13697400142439178, 0.008158290043142755)\n",
      "All Variables pearson (0.32826683351975633, 8.53745794331336e-11)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_stacked_regression('stress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "id": "0773a6a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------SOCIAL MEDIA---------\n",
      "Baseline R-square adj 0.13044553100333411\n",
      "Life Events R-square adj 0.018523183700833945\n",
      "All Variables R-square adj 0.13803662946251982\n",
      "\n",
      "   df_resid          ssr  df_diff     ss_diff          F  Pr(>F)\n",
      "0     653.0  1046.111369      0.0         NaN        NaN     NaN\n",
      "1     657.0  1187.991585     -4.0 -141.880216  19.616154     NaN\n",
      "   df_resid          ssr  df_diff    ss_diff         F    Pr(>F)\n",
      "0     653.0  1046.111369      0.0        NaN       NaN       NaN\n",
      "1     641.0  1017.922677     12.0  28.188692  1.479234  0.126886\n",
      "   df_resid          ssr  df_diff     ss_diff         F        Pr(>F)\n",
      "0     657.0  1187.991585      0.0         NaN       NaN           NaN\n",
      "1     641.0  1017.922677     16.0  170.068908  6.693422  3.318120e-14\n",
      "\n",
      "Baseline pearson (0.393488744284707, 2.5579854045044186e-06)\n",
      "Life Events pearson (0.0727072481263923, 0.40379211085773664)\n",
      "All Variables pearson (0.3351792277116428, 7.533688434925699e-05)\n",
      "\n",
      "---------SURVEY---------\n",
      "Baseline R-square adj 0.08164183887495735\n",
      "Life Events R-square adj 0.015054833343084995\n",
      "All Variables R-square adj 0.11315026643077875\n",
      "\n",
      "   df_resid          ssr  df_diff     ss_diff          F  Pr(>F)\n",
      "0    1193.0  1703.583601      0.0         NaN        NaN     NaN\n",
      "1    1197.0  1833.230699     -4.0 -129.647098  21.163127     NaN\n",
      "   df_resid          ssr  df_diff    ss_diff         F        Pr(>F)\n",
      "0    1193.0  1703.583601      0.0        NaN       NaN           NaN\n",
      "1    1181.0  1628.586592     12.0  74.997009  4.532124  3.504323e-07\n",
      "   df_resid          ssr  df_diff     ss_diff         F        Pr(>F)\n",
      "0    1197.0  1833.230699      0.0         NaN       NaN           NaN\n",
      "1    1181.0  1628.586592     16.0  204.644107  9.275094  5.470061e-22\n",
      "\n",
      "Baseline pearson (0.21911565186939475, 0.0005973794343552469)\n",
      "Life Events pearson (0.1224402685621168, 0.05716799540785801)\n",
      "All Variables pearson (0.25068518112296967, 8.057118742213032e-05)\n",
      "\n",
      "---------SOCIAL MEDIA + SURVEY---------\n",
      "Baseline R-square adj 0.301992542400751\n",
      "Life Events R-square adj 0.14362586251409515\n",
      "All Variables R-square adj 0.3826710900944126\n",
      "\n",
      "   df_resid         ssr  df_diff    ss_diff         F  Pr(>F)\n",
      "0     146.0  249.348775      0.0        NaN       NaN     NaN\n",
      "1     139.0  291.254513      7.0 -41.905738 -2.857048     1.0\n",
      "   df_resid         ssr  df_diff    ss_diff         F    Pr(>F)\n",
      "0     146.0  249.348775      0.0        NaN       NaN       NaN\n",
      "1     123.0  185.787310     23.0  63.561465  1.829596  0.019061\n",
      "   df_resid         ssr  df_diff     ss_diff         F    Pr(>F)\n",
      "0     139.0  291.254513      0.0         NaN       NaN       NaN\n",
      "1     123.0  185.787310     16.0  105.467203  4.364018  0.000001\n",
      "\n",
      "Baseline pearson (0.5211232613529782, 0.001873222487881202)\n",
      "Life Events pearson (0.3835281951451731, 0.02757519046739907)\n",
      "All Variables pearson (0.48043945247605724, 0.004656740974947088)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_all_regression('sleep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "id": "5d951479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------SOCIAL MEDIA + SURVEY---------\n",
      "Baseline R-square adj 0.06676800727341381\n",
      "Life Events R-square adj 0.010005883226233636\n",
      "All Variables R-square adj 0.07987385576517281\n",
      "\n",
      "   df_resid          ssr  df_diff     ss_diff          F  Pr(>F)\n",
      "0    1750.0  2581.683798      0.0         NaN        NaN     NaN\n",
      "1    1753.0  2743.404911     -3.0 -161.721113  34.445895     NaN\n",
      "   df_resid          ssr  df_diff    ss_diff         F    Pr(>F)\n",
      "0    1750.0  2581.683798      0.0        NaN       NaN       NaN\n",
      "1    1736.0  2525.064485     14.0  56.619314  2.780442  0.000416\n",
      "   df_resid          ssr  df_diff     ss_diff         F        Pr(>F)\n",
      "0    1753.0  2743.404911      0.0         NaN       NaN           NaN\n",
      "1    1736.0  2525.064485     17.0  218.340426  8.830036  2.791074e-22\n",
      "\n",
      "Baseline pearson (0.28215034764799257, 6.665717665069504e-08)\n",
      "Life Events pearson (0.0905279642863686, 0.08899125194419409)\n",
      "All Variables pearson (0.2857789686893533, 4.4391504801327584e-08)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_stacked_regression('sleep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "id": "82a4a682",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------SOCIAL MEDIA---------\n",
      "Baseline R-square adj 0.1675543210113799\n",
      "Life Events R-square adj 0.05759560455052348\n",
      "All Variables R-square adj 0.20597707056754688\n",
      "\n",
      "   df_resid         ssr  df_diff  ss_diff          F  Pr(>F)\n",
      "0     722.0  242.774207      0.0      NaN        NaN     NaN\n",
      "1     726.0  276.365207     -4.0  -33.591  22.060543     NaN\n",
      "   df_resid         ssr  df_diff    ss_diff         F    Pr(>F)\n",
      "0     722.0  242.774207      0.0        NaN       NaN       NaN\n",
      "1     710.0  227.719822     12.0  15.054385  3.911463  0.000008\n",
      "   df_resid         ssr  df_diff    ss_diff         F        Pr(>F)\n",
      "0     726.0  276.365207      0.0        NaN       NaN           NaN\n",
      "1     710.0  227.719822     16.0  48.645385  9.479363  1.241943e-21\n",
      "\n",
      "Baseline pearson (0.35812996941414127, 7.857072126800395e-06)\n",
      "Life Events pearson (0.14166457652616188, 0.08589077676005877)\n",
      "All Variables pearson (0.3569311324607578, 8.46740774095825e-06)\n",
      "\n",
      "---------SURVEY---------\n",
      "Baseline R-square adj 0.2161289450567555\n",
      "Life Events R-square adj 0.03379977071500295\n",
      "All Variables R-square adj 0.232510784216577\n",
      "\n",
      "   df_resid         ssr  df_diff     ss_diff          F  Pr(>F)\n",
      "0    1253.0  447.859244      0.0         NaN        NaN     NaN\n",
      "1    1257.0  553.794015     -4.0 -105.934771  60.112606     NaN\n",
      "   df_resid         ssr  df_diff    ss_diff         F    Pr(>F)\n",
      "0    1253.0  447.859244      0.0        NaN       NaN       NaN\n",
      "1    1241.0  434.300078     12.0  13.559166  3.228744  0.000137\n",
      "   df_resid         ssr  df_diff     ss_diff          F        Pr(>F)\n",
      "0    1257.0  553.794015      0.0         NaN        NaN           NaN\n",
      "1    1241.0  434.300078     16.0  119.493937  21.340656  5.288680e-55\n",
      "\n",
      "Baseline pearson (0.35971658040731613, 3.5597424850373164e-09)\n",
      "Life Events pearson (0.22522105128311704, 0.00029651473375498884)\n",
      "All Variables pearson (0.387894424475521, 1.51339273887542e-10)\n",
      "\n",
      "---------SOCIAL MEDIA + SURVEY---------\n",
      "Baseline R-square adj 0.17017090555118564\n",
      "Life Events R-square adj 0.25256580368586623\n",
      "All Variables R-square adj 0.36901682627247134\n",
      "\n",
      "   df_resid         ssr  df_diff    ss_diff         F    Pr(>F)\n",
      "0     179.0  105.452648      0.0        NaN       NaN       NaN\n",
      "1     172.0   91.267722      7.0  14.184926  3.818917  0.000698\n",
      "   df_resid         ssr  df_diff    ss_diff         F    Pr(>F)\n",
      "0     179.0  105.452648      0.0        NaN       NaN       NaN\n",
      "1     156.0   69.880850     23.0  35.571798  3.452585  0.000002\n",
      "   df_resid        ssr  df_diff    ss_diff         F    Pr(>F)\n",
      "0     172.0  91.267722      0.0        NaN       NaN       NaN\n",
      "1     156.0  69.880850     16.0  21.386872  2.983965  0.000238\n",
      "\n",
      "Baseline pearson (0.44356245756112334, 0.004146622993077272)\n",
      "Life Events pearson (0.3833932309416662, 0.014605031228188157)\n",
      "All Variables pearson (0.5112932485811517, 0.0007470664282561029)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_all_regression('anxiety')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "id": "7da27f9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------SOCIAL MEDIA + SURVEY---------\n",
      "Baseline R-square adj 0.19934215456241433\n",
      "Life Events R-square adj 0.061474698726827315\n",
      "All Variables R-square adj 0.2343837973835946\n",
      "\n",
      "   df_resid         ssr  df_diff     ss_diff          F  Pr(>F)\n",
      "0    1843.0  611.717138      0.0         NaN        NaN     NaN\n",
      "1    1846.0  718.217579     -3.0 -106.500441  91.244316     NaN\n",
      "   df_resid         ssr  df_diff    ss_diff         F        Pr(>F)\n",
      "0    1843.0  611.717138      0.0        NaN       NaN           NaN\n",
      "1    1829.0  580.501265     14.0  31.215873  7.025189  2.625508e-14\n",
      "   df_resid         ssr  df_diff     ss_diff          F        Pr(>F)\n",
      "0    1846.0  718.217579      0.0         NaN        NaN           NaN\n",
      "1    1829.0  580.501265     17.0  137.716315  25.523898  1.459804e-72\n",
      "\n",
      "Baseline pearson (0.38164032800144354, 2.409808294259408e-14)\n",
      "Life Events pearson (0.2352883428500234, 4.488739267686155e-06)\n",
      "All Variables pearson (0.4187368687361116, 3.1701759132132746e-17)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_stacked_regression('anxiety')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "id": "b9179ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stress_level(row):\n",
    "    if row['stress'] >= 4.0:\n",
    "        return 'High'\n",
    "    elif row['stress'] <= 2.0:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'\n",
    "    \n",
    "def sleep_level(row):\n",
    "    if row['sleep'] >= 8.0:\n",
    "        return 'High'\n",
    "    elif row['sleep'] <= 4.0:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'\n",
    "    \n",
    "def anxiety_level(row):\n",
    "    if row['anxiety'] >= 3.0:\n",
    "        return 'High'\n",
    "    elif row['anxiety'] <= 2.0:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "id": "cfc14098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_social_media_data = pd.read_csv('data/Superimposed/Facebook Data For Life Events-Combined - FB Data.csv')\n",
    "df_social_media_data = df_social_media_data[['snapshot_id', 'created_time', 'Text', 'final_life_event_category_2', 'ended/ongoing']]\n",
    "df_social_media_data = df_social_media_data.replace({'PostiveMove':'PositiveMove', 'Negative Move':'NegativeMove'})\n",
    "df_social_media_data = df_social_media_data.drop(df_social_media_data[((df_social_media_data['final_life_event_category_2'].isnull() == True))].index)\n",
    "df_social_media_data['created_date'] = pd.to_datetime(df_social_media_data['created_time'], format = '%Y-%m-%d %H:%M:%S').dt.date\n",
    "df_social_media_data['valence'] = df_social_media_data.apply(compute_sentiment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "id": "a95a3de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>created_time</th>\n",
       "      <th>Text</th>\n",
       "      <th>final_life_event_category_2</th>\n",
       "      <th>ended/ongoing</th>\n",
       "      <th>created_date</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12840161237957324034</td>\n",
       "      <td>2018-03-16 17:30:07</td>\n",
       "      <td>Next stop....Seoul, South Korea!</td>\n",
       "      <td>Vacation</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-03-16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11156103277197680506</td>\n",
       "      <td>2018-07-31 13:10:19</td>\n",
       "      <td>¡Hola Barcelona! We are in you 😂</td>\n",
       "      <td>Vacation</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11329586216091316598</td>\n",
       "      <td>2018-09-08 2:11:19</td>\n",
       "      <td>Family beach vacation has begun!!  Day one was...</td>\n",
       "      <td>Vacation</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-09-08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13543308681200334026</td>\n",
       "      <td>2018-07-22 16:05:38</td>\n",
       "      <td>🛫 to sea, sand and scuba</td>\n",
       "      <td>Vacation</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12676581714475000627</td>\n",
       "      <td>2018-04-25 23:55:28</td>\n",
       "      <td>Off to Dublin!! #fernweh #wanderlust #selfdisc...</td>\n",
       "      <td>Vacation</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14181</th>\n",
       "      <td>12971652107468080963</td>\n",
       "      <td>2018-08-26 18:56:39</td>\n",
       "      <td>Anthony is officially a middle schooler!  His ...</td>\n",
       "      <td>ChangedSchool</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-08-26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14182</th>\n",
       "      <td>12971652107468080963</td>\n",
       "      <td>2018-08-26 22:25:53</td>\n",
       "      <td>On Friday we closed on our home and it brings ...</td>\n",
       "      <td>NeutralMove</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-08-26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14184</th>\n",
       "      <td>12971652107468080963</td>\n",
       "      <td>2018-10-03 0:45:06</td>\n",
       "      <td>These handsome fellas!!!  Adam: Boulder High S...</td>\n",
       "      <td>ChangedSchool</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14188</th>\n",
       "      <td>12201010563123762054</td>\n",
       "      <td>2018-03-02 14:27:59</td>\n",
       "      <td>Our house is on the market! If you know anyone...</td>\n",
       "      <td>NeutralMove</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14189</th>\n",
       "      <td>12201010563123762054</td>\n",
       "      <td>2018-10-20 14:32:16</td>\n",
       "      <td>Ran the 10k this morning at the #KCMarathon wi...</td>\n",
       "      <td>Local</td>\n",
       "      <td>ended</td>\n",
       "      <td>2018-10-20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1975 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                snapshot_id         created_time  \\\n",
       "0      12840161237957324034  2018-03-16 17:30:07   \n",
       "1      11156103277197680506  2018-07-31 13:10:19   \n",
       "2      11329586216091316598   2018-09-08 2:11:19   \n",
       "3      13543308681200334026  2018-07-22 16:05:38   \n",
       "4      12676581714475000627  2018-04-25 23:55:28   \n",
       "...                     ...                  ...   \n",
       "14181  12971652107468080963  2018-08-26 18:56:39   \n",
       "14182  12971652107468080963  2018-08-26 22:25:53   \n",
       "14184  12971652107468080963   2018-10-03 0:45:06   \n",
       "14188  12201010563123762054  2018-03-02 14:27:59   \n",
       "14189  12201010563123762054  2018-10-20 14:32:16   \n",
       "\n",
       "                                                    Text  \\\n",
       "0                       Next stop....Seoul, South Korea!   \n",
       "1                       ¡Hola Barcelona! We are in you 😂   \n",
       "2      Family beach vacation has begun!!  Day one was...   \n",
       "3                               🛫 to sea, sand and scuba   \n",
       "4      Off to Dublin!! #fernweh #wanderlust #selfdisc...   \n",
       "...                                                  ...   \n",
       "14181  Anthony is officially a middle schooler!  His ...   \n",
       "14182  On Friday we closed on our home and it brings ...   \n",
       "14184  These handsome fellas!!!  Adam: Boulder High S...   \n",
       "14188  Our house is on the market! If you know anyone...   \n",
       "14189  Ran the 10k this morning at the #KCMarathon wi...   \n",
       "\n",
       "      final_life_event_category_2 ended/ongoing created_date  valence  \n",
       "0                        Vacation       ongoing   2018-03-16        0  \n",
       "1                        Vacation       ongoing   2018-07-31        1  \n",
       "2                        Vacation       ongoing   2018-09-08        0  \n",
       "3                        Vacation       ongoing   2018-07-22        0  \n",
       "4                        Vacation       ongoing   2018-04-25        0  \n",
       "...                           ...           ...          ...      ...  \n",
       "14181               ChangedSchool       ongoing   2018-08-26        0  \n",
       "14182                 NeutralMove       ongoing   2018-08-26        1  \n",
       "14184               ChangedSchool       ongoing   2018-10-03        1  \n",
       "14188                 NeutralMove       ongoing   2018-03-02        1  \n",
       "14189                       Local         ended   2018-10-20        0  \n",
       "\n",
       "[1975 rows x 7 columns]"
      ]
     },
     "execution_count": 1126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_social_media_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "id": "62abee1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6333333333333333"
      ]
     },
     "execution_count": 1154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_per_individual = df_dailies.groupby(['snapshot_id']).median().reset_index()[['snapshot_id','stress', 'sleep', 'anxiety']]\n",
    "\n",
    "median_per_individual[median_per_individual['snapshot_id'] == 9233950727806111817]['stress'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "id": "2c0c4d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1285\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>stress</th>\n",
       "      <th>sleep</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>shipley_vocab</th>\n",
       "      <th>shipley_abs</th>\n",
       "      <th>...</th>\n",
       "      <th>created_time</th>\n",
       "      <th>Text</th>\n",
       "      <th>final_life_event_category_2</th>\n",
       "      <th>ended/ongoing</th>\n",
       "      <th>created_date</th>\n",
       "      <th>valence</th>\n",
       "      <th>stress_imputed</th>\n",
       "      <th>sleep_imputed</th>\n",
       "      <th>anxiety_imputed</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12840161237957324034</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>7.60</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-03-16 17:30:07</td>\n",
       "      <td>Next stop....Seoul, South Korea!</td>\n",
       "      <td>Vacation</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-03-16</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12840161237957324034</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>7.60</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-03-17 10:23:39</td>\n",
       "      <td>Dinner on the airplane, Bibimbap! Oh and by th...</td>\n",
       "      <td>Vacation</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11156103277197680506</td>\n",
       "      <td>2018</td>\n",
       "      <td>31</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-07-31 13:10:19</td>\n",
       "      <td>¡Hola Barcelona! We are in you 😂</td>\n",
       "      <td>Vacation</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11156103277197680506</td>\n",
       "      <td>2018</td>\n",
       "      <td>31</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-07-31 9:42:32</td>\n",
       "      <td>A good friend will help you find gluten free r...</td>\n",
       "      <td>Vacation</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11156103277197680506</td>\n",
       "      <td>2018</td>\n",
       "      <td>31</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-07-31 19:44:05</td>\n",
       "      <td>The Sagrada Família is like being inside a rai...</td>\n",
       "      <td>Vacation</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>12971652107468080963</td>\n",
       "      <td>2018</td>\n",
       "      <td>34</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-08-26 18:56:39</td>\n",
       "      <td>Anthony is officially a middle schooler!  His ...</td>\n",
       "      <td>ChangedSchool</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-08-26</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>12971652107468080963</td>\n",
       "      <td>2018</td>\n",
       "      <td>34</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-08-26 22:25:53</td>\n",
       "      <td>On Friday we closed on our home and it brings ...</td>\n",
       "      <td>NeutralMove</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-08-26</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>12971652107468080963</td>\n",
       "      <td>2018</td>\n",
       "      <td>40</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-10-03 0:45:06</td>\n",
       "      <td>These handsome fellas!!!  Adam: Boulder High S...</td>\n",
       "      <td>ChangedSchool</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>12201010563123762054</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>7.50</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>36.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-03-02 14:27:59</td>\n",
       "      <td>Our house is on the market! If you know anyone...</td>\n",
       "      <td>NeutralMove</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>12201010563123762054</td>\n",
       "      <td>2018</td>\n",
       "      <td>42</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-10-20 14:32:16</td>\n",
       "      <td>Ran the 10k this morning at the #KCMarathon wi...</td>\n",
       "      <td>Local</td>\n",
       "      <td>ended</td>\n",
       "      <td>2018-10-20</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1975 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               snapshot_id  year  week    stress  sleep   anxiety   age  \\\n",
       "0     12840161237957324034  2018    11  2.285714   7.60  2.285714  36.0   \n",
       "1     12840161237957324034  2018    11  2.285714   7.60  2.285714  36.0   \n",
       "2     11156103277197680506  2018    31  2.000000   8.75  1.000000   NaN   \n",
       "3     11156103277197680506  2018    31  2.000000   8.75  1.000000   NaN   \n",
       "4     11156103277197680506  2018    31  2.000000   8.75  1.000000   NaN   \n",
       "...                    ...   ...   ...       ...    ...       ...   ...   \n",
       "1970  12971652107468080963  2018    34  2.000000   8.00  1.000000   NaN   \n",
       "1971  12971652107468080963  2018    34  2.000000   8.00  1.000000   NaN   \n",
       "1972  12971652107468080963  2018    40  2.000000   8.00  1.000000   NaN   \n",
       "1973  12201010563123762054  2018     9  2.666667   7.50  1.833333  31.0   \n",
       "1974  12201010563123762054  2018    42  2.000000   8.00  1.000000   NaN   \n",
       "\n",
       "      gender  shipley_vocab  shipley_abs  ...         created_time  \\\n",
       "0     Female           27.0          9.0  ...  2018-03-16 17:30:07   \n",
       "1     Female           27.0          9.0  ...  2018-03-17 10:23:39   \n",
       "2        NaN            NaN          NaN  ...  2018-07-31 13:10:19   \n",
       "3        NaN            NaN          NaN  ...   2018-07-31 9:42:32   \n",
       "4        NaN            NaN          NaN  ...  2018-07-31 19:44:05   \n",
       "...      ...            ...          ...  ...                  ...   \n",
       "1970     NaN            NaN          NaN  ...  2018-08-26 18:56:39   \n",
       "1971     NaN            NaN          NaN  ...  2018-08-26 22:25:53   \n",
       "1972     NaN            NaN          NaN  ...   2018-10-03 0:45:06   \n",
       "1973    Male           36.0         20.0  ...  2018-03-02 14:27:59   \n",
       "1974     NaN            NaN          NaN  ...  2018-10-20 14:32:16   \n",
       "\n",
       "                                                   Text  \\\n",
       "0                      Next stop....Seoul, South Korea!   \n",
       "1     Dinner on the airplane, Bibimbap! Oh and by th...   \n",
       "2                      ¡Hola Barcelona! We are in you 😂   \n",
       "3     A good friend will help you find gluten free r...   \n",
       "4     The Sagrada Família is like being inside a rai...   \n",
       "...                                                 ...   \n",
       "1970  Anthony is officially a middle schooler!  His ...   \n",
       "1971  On Friday we closed on our home and it brings ...   \n",
       "1972  These handsome fellas!!!  Adam: Boulder High S...   \n",
       "1973  Our house is on the market! If you know anyone...   \n",
       "1974  Ran the 10k this morning at the #KCMarathon wi...   \n",
       "\n",
       "      final_life_event_category_2  ended/ongoing  created_date  valence  \\\n",
       "0                        Vacation        ongoing    2018-03-16        0   \n",
       "1                        Vacation        ongoing    2018-03-17        0   \n",
       "2                        Vacation        ongoing    2018-07-31        1   \n",
       "3                        Vacation        ongoing    2018-07-31        1   \n",
       "4                        Vacation        ongoing    2018-07-31        1   \n",
       "...                           ...            ...           ...      ...   \n",
       "1970                ChangedSchool        ongoing    2018-08-26        0   \n",
       "1971                  NeutralMove        ongoing    2018-08-26        1   \n",
       "1972                ChangedSchool        ongoing    2018-10-03        1   \n",
       "1973                  NeutralMove        ongoing    2018-03-02        1   \n",
       "1974                        Local          ended    2018-10-20        0   \n",
       "\n",
       "      stress_imputed  sleep_imputed  anxiety_imputed   test  \n",
       "0              False          False            False  False  \n",
       "1              False          False            False  False  \n",
       "2               True           True             True  False  \n",
       "3               True           True             True  False  \n",
       "4               True           True             True  False  \n",
       "...              ...            ...              ...    ...  \n",
       "1970            True           True             True  False  \n",
       "1971            True           True             True  False  \n",
       "1972            True           True             True  False  \n",
       "1973           False          False            False  False  \n",
       "1974            True           True             True  False  \n",
       "\n",
       "[1975 rows x 30 columns]"
      ]
     },
     "execution_count": 1169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dailies = load_dailies_data()\n",
    "median_per_individual = df_dailies.groupby(['snapshot_id']).median().reset_index()[['snapshot_id','stress', 'sleep', 'anxiety']]\n",
    "df_demographics = load_demographics_data()\n",
    "\n",
    "df_social_media_data['year'] = df_social_media_data.apply(calculate_year, columnName='created_date', axis=1)\n",
    "df_social_media_data['week'] = df_social_media_data.apply(calculate_week, columnName='created_date', axis=1)\n",
    "\n",
    "df_dailies['year'] = df_dailies.apply(calculate_year, columnName='day', axis=1)\n",
    "df_dailies['week'] = df_dailies.apply(calculate_week, columnName='day', axis=1)\n",
    "df_dailies = df_dailies[['snapshot_id', 'stress', 'sleep', 'anxiety', 'year', 'week']]\n",
    "df_dailies = df_dailies.groupby(['snapshot_id', 'year', 'week']).mean().reset_index()\n",
    "\n",
    "merged_data = pd.merge(df_dailies, df_demographics, how=\"inner\", on=\"snapshot_id\")\n",
    "merged_data_test = pd.merge(merged_data, df_social_media_data, how=\"right\", on=[\"snapshot_id\", \"year\", \"week\"])\n",
    "\n",
    "merged_data_test['stress_imputed'] = False\n",
    "merged_data_test['sleep_imputed'] = False\n",
    "merged_data_test['anxiety_imputed'] = False\n",
    "\n",
    "for i, row in merged_data_test.iterrows():\n",
    "    if row['stress'] != row['stress']:\n",
    "        merged_data_test._set_value(i,'stress',median_per_individual[median_per_individual['snapshot_id'] == row['snapshot_id']]['stress'].values[0])\n",
    "        merged_data_test._set_value(i,'stress_imputed',True)\n",
    "    if row['sleep'] != row['sleep']:\n",
    "        merged_data_test._set_value(i,'sleep',median_per_individual[median_per_individual['snapshot_id'] == row['snapshot_id']]['sleep'].values[0])\n",
    "        merged_data_test._set_value(i,'sleep_imputed',True)\n",
    "    if row['anxiety'] != row['anxiety']:\n",
    "        merged_data_test._set_value(i,'anxiety',median_per_individual[median_per_individual['snapshot_id'] == row['snapshot_id']]['anxiety'].values[0])\n",
    "        merged_data_test._set_value(i,'anxiety_imputed',True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "id": "a69c960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dailies = load_dailies_data()\n",
    "df_demographics = load_demographics_data()\n",
    "\n",
    "df_social_media_data['year'] = df_social_media_data.apply(calculate_year, columnName='created_date', axis=1)\n",
    "df_social_media_data['week'] = df_social_media_data.apply(calculate_week, columnName='created_date', axis=1)\n",
    "\n",
    "df_dailies['year'] = df_dailies.apply(calculate_year, columnName='day', axis=1)\n",
    "df_dailies['week'] = df_dailies.apply(calculate_week, columnName='day', axis=1)\n",
    "df_dailies = df_dailies[['snapshot_id', 'stress', 'sleep', 'anxiety', 'year', 'week']]\n",
    "df_dailies = df_dailies.groupby(['snapshot_id', 'year', 'week']).mean().reset_index()\n",
    "\n",
    "merged_data = pd.merge(df_dailies, df_demographics, how=\"inner\", on=\"snapshot_id\")\n",
    "merged_data_sm = pd.merge(merged_data, df_social_media_data, how=\"inner\", on=[\"snapshot_id\", \"year\", \"week\"])\n",
    "\n",
    "merged_data_sm['stress_imputed'] = False\n",
    "merged_data_sm['sleep_imputed'] = False\n",
    "merged_data_sm['anxiety_imputed'] = False\n",
    "\n",
    "for i, row in merged_data_test.iterrows():\n",
    "    if row['stress'] != row['stress']:\n",
    "        merged_data_sm._set_value(i,'stress',median_per_individual[median_per_individual['snapshot_id'] == row['snapshot_id']]['stress'].values[0])\n",
    "        merged_data_sm._set_value(i,'stress_imputed',True)\n",
    "    if row['sleep'] != row['sleep']:\n",
    "        merged_data_sm._set_value(i,'sleep',median_per_individual[median_per_individual['snapshot_id'] == row['snapshot_id']]['sleep'].values[0])\n",
    "        merged_data_sm._set_value(i,'sleep_imputed',True)\n",
    "    if row['anxiety'] != row['anxiety']:\n",
    "        merged_data_sm._set_value(i,'anxiety',median_per_individual[median_per_individual['snapshot_id'] == row['snapshot_id']]['anxiety'].values[0])\n",
    "        merged_data_sm._set_value(i,'anxiety_imputed',True)\n",
    "\n",
    "\n",
    "merged_data_sm = merged_data_sm[['snapshot_id', 'Text', 'stress', 'sleep', 'anxiety', 'stress_imputed', 'sleep_imputed', 'anxiety_imputed']]\n",
    "merged_data_sm\n",
    "\n",
    "# merged_data_sm.to_csv('Linear Regression/Calendar Week/social_media_post_mental_health_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "id": "07cfad82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /nethome/sagarwal420/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def generate_tokens(words):\n",
    "#     words = remove_emoji(words)\n",
    "    words = clean_text(words)\n",
    "    words = words.split()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        if len(word) <= 2:\n",
    "            continue\n",
    "        if word not in stopwords:\n",
    "            stems.append(stemmer.stem(word))\n",
    "    return stems\n",
    "\n",
    "def remove_emoji(inputString):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    u\"\\U0001f926-\\U0001f937\"\n",
    "    u'\\U00010000-\\U0010ffff'\n",
    "    u\"\\u200d\"\n",
    "    u\"\\u2640-\\u2642\"\n",
    "    u\"\\u2600-\\u2B55\"\n",
    "    u\"\\u23cf\"\n",
    "    u\"\\u23e9\"\n",
    "    u\"\\u231a\"\n",
    "    u\"\\u3030\"\n",
    "    u\"\\ufe0f\"\n",
    "    u\"\\u2069\"\n",
    "    u\"\\u2066\"\n",
    "    u\"\\u200c\"\n",
    "    u\"\\u2068\"\n",
    "    u\"\\u2067\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', inputString)\n",
    "\n",
    "def clean_text(x):\n",
    "    x = re.sub(r'\\d+', '', x)\n",
    "    x = \"\".join([char.lower() for char in x if char not in string.punctuation])\n",
    "    x = re.sub('\\s+', ' ', x).strip()\n",
    "    return del_punctuations(remove_emoji(x))\n",
    "\n",
    "\n",
    "def del_punctuations(text):\n",
    "    return text.translate(str.maketrans('','',string.punctuation))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "id": "da4e661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_500 = TfidfVectorizer(tokenizer = generate_tokens, ngram_range=(1, 2), max_features=500)\n",
    "vector_500 = vectorizer_500.fit_transform(merged_data_sm['Text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "id": "55d7e6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['chicago', 'friend', 'gone', 'full', 'fun', 'futur', 'game',\n",
       "       'garmin', 'get', 'get marri', 'girl', 'girlstrip', 'give', 'glad',\n",
       "       'go', 'god', 'yesterday', 'guess', 'good', 'goodby'], dtype='<U23')"
      ]
     },
     "execution_count": 1066,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorizer_500.get_feature_names()\n",
    "\n",
    "tfidf_sorting = np.argsort(vector_500).flatten()[::-1]\n",
    "\n",
    "n = 20\n",
    "top_n = np.array(vectorizer_500.get_feature_names())[tfidf_sorting][:n]\n",
    "top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "id": "f7864bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_1000 = TfidfVectorizer(tokenizer = generate_tokens, ngram_range=(1, 2), max_features=1000)\n",
    "vector_1000 = vectorizer_1000.fit_transform(merged_data_sm['Text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "id": "8b497b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['illinoi', 'chicago', 'friend', 'gather', 'girl bed', 'girl',\n",
       "       'get sugar', 'get readi', 'get old', 'get meet', 'get med',\n",
       "       'get marri', 'get easier', 'get back', 'get', 'gave', 'garag',\n",
       "       'garmin', 'give', 'gap'], dtype='<U23')"
      ]
     },
     "execution_count": 1070,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorizer_500.get_feature_names()\n",
    "\n",
    "tfidf_sorting = np.argsort(vector_1000).flatten()[::-1]\n",
    "\n",
    "n = 20\n",
    "top_n = np.array(vectorizer_1000.get_feature_names())[tfidf_sorting][:n]\n",
    "top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "id": "b2cf25d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_top_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1031-cfb37e80839d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizer_1000\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_top_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_top_feature_names'"
     ]
    }
   ],
   "source": [
    "vectorizer_1000.get_top_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
