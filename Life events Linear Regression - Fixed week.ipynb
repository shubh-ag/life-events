{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "10482a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install statsmodels\n",
    "# !pip3 install vaderSentiment\n",
    "# !pip3 install nbconvert\n",
    "# !pip3 install tabulate\n",
    "# !pip3 install --upgrade scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "bc4ee099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "from statistics import mean\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.gof import chisquare as chisquare\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "import csv\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "7db6c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'Linear Regression/Calendar Week/'\n",
    "INTERMEDIATE_DATA = BASE_DIR+'temporary/'\n",
    "MODEL_DIR = BASE_DIR+'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "b39d1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.datetime.strptime(\"2020-08-22\", \"%Y-%m-%d\").date()\n",
    "\n",
    "def convertToTime(row, columnName):\n",
    "    return datetime.datetime.strptime(row[columnName], \"%Y-%m-%d\").date()\n",
    "\n",
    "def convertToDate(row, columnName):\n",
    "    return datetime.datetime.strptime(row[columnName], \"%Y-%m-%d %H:%M:%S\").date()\n",
    "    \n",
    "def update_end_date(row, columnName, latestDate):\n",
    "    if row[columnName] == row[columnName]:\n",
    "        return row[columnName]\n",
    "    else:\n",
    "        return latestDate\n",
    "\n",
    "def getDays(row, beginColumnName, endColumnName):\n",
    "    v = datetime.datetime.strptime(str(row[endColumnName]), \"%Y-%m-%d\").date() - datetime.datetime.strptime(row[beginColumnName], \"%Y-%m-%d\").date()\n",
    "    return v.days\n",
    "\n",
    "def calculate_recency(row, columnName):\n",
    "    return (current_date - datetime.datetime.strptime(str(row[columnName]), \"%Y-%m-%d\").date()).days\n",
    "\n",
    "def lookup_index(row, columnName, array):\n",
    "    if(row[columnName] not in array):\n",
    "        return -1\n",
    "    return array.index(row[columnName]) + 1\n",
    "\n",
    "def colour_life_events(row):\n",
    "    colours = {'personal':'lightcoral', 'health':'orange', 'work':'lightgreen', 'financial':'teal', 'weather':'blueviolet', 'societal':'navy','other':'skyblue'}\n",
    "    return colours[row['life_event_type']]\n",
    "\n",
    "def remove_rows(base_df, other_df):\n",
    "    modified_df = other_df.drop(other_df[other_df['snapshot_id'] not in base_df['snapshot_id'].values].index)\n",
    "    return modified_df\n",
    "\n",
    "def fix_signficance(row):\n",
    "    if('significance' in row['valence']):\n",
    "        return row['valence']\n",
    "    else:\n",
    "        return row['significance']\n",
    "\n",
    "def fix_valence(row):\n",
    "    if('significance' in row['significance']):\n",
    "        return row['valence']\n",
    "    else:\n",
    "        return row['significance']  \n",
    "\n",
    "def get_broad_category(row, categories, column_name):\n",
    "    if(row[column_name] in categories):\n",
    "        return categories[row[column_name]]\n",
    "    return \"UNKNOWN\"\n",
    "    \n",
    "def compute_sentiment(row):\n",
    "    post = row['Text']\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(post)\n",
    "    sentiment = 0\n",
    "    if (vs[\"neu\"]>0.8):\n",
    "        sentiment = 0\n",
    "    elif (vs[\"pos\"]==vs[\"neg\"]):\n",
    "        sentiment = 0\n",
    "    elif (vs[\"pos\"]>vs[\"neg\"]):\n",
    "        sentiment = 1\n",
    "    elif (vs[\"neg\"]>vs[\"neu\"]):\n",
    "        sentiment = -1\n",
    "    return sentiment\n",
    "\n",
    "def convert_valence_to_sentiment(row):\n",
    "    valence = row['valence']\n",
    "    retVal = 0\n",
    "    if (valence == 'Neither Positive or Negative'):\n",
    "        retVal = 0\n",
    "    elif(\"Positive\" in valence):\n",
    "        retVal = 1\n",
    "    elif(\"Negative\" in valence):\n",
    "        retVal = -1\n",
    "    return retVal\n",
    "\n",
    "def update_status(row, columnName):\n",
    "    if row[columnName] == row[columnName]:\n",
    "        if \"ongoing\" in row[columnName].lower():\n",
    "            return \"Ongoing\"\n",
    "        return row[columnName]\n",
    "    else:\n",
    "        return \"Ended\"\n",
    "\n",
    "def update_education_level(row, columnName):\n",
    "    if 'college' in row[columnName].lower():\n",
    "        return \"College Degree\"\n",
    "    elif 'doctoral' in row[columnName].lower():\n",
    "        return \"Doctoral Degree\"\n",
    "    elif 'master' in row[columnName].lower() or 'grad' in row[columnName].lower():\n",
    "        return \"Graduate Degree\"\n",
    "    elif 'hs' in row[columnName].lower() or 'high school' in row[columnName].lower():\n",
    "        return \"High School\"\n",
    "    \n",
    "    return row[columnName]\n",
    "\n",
    "def add_avg_date_based_dependent_variable(row, weekly_data):\n",
    "    date = datetime.datetime.strptime(str(row['created_date']), \"%Y-%m-%d\").date()\n",
    "    year, week = (date.isocalendar()[0], date.isocalendar()[1])\n",
    "    \n",
    "    if (year,week) in weekly_data[row['snapshot_id']]:\n",
    "        return weekly_data[row['snapshot_id']][(year, week)]\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "4331d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_variables             = 'shipley_vocab + shipley_abs + openness + conscientiousness + extraversion + agreeableness + neuroticism + pos_affect + neg_affect + stai_trait + education_level + psqi + age + gender'\n",
    "\n",
    "sr_life_event_variables       = 'Anticipation + LifeEventFamily + valence + recency + status + Intimacy + Scope + significance_label'\n",
    "sm_life_event_variables       = 'Anticipation + LifeEventFamily + valence + recency + status + Intimacy + Scope + SignificanceRank'\n",
    "combined_life_event_variables = 'Anticipation + LifeEventFamily + valence + recency + status + Intimacy + Scope + data_type'\n",
    "\n",
    "sr_all_variables     = 'shipley_vocab + shipley_abs + openness + conscientiousness + extraversion + agreeableness + neuroticism + pos_affect + neg_affect + stai_trait + education_level + psqi + age + gender + Anticipation + LifeEventFamily + valence + recency + status + Intimacy + Scope + significance_label'\n",
    "sm_all_variables     = 'shipley_vocab + shipley_abs + openness + conscientiousness + extraversion + agreeableness + neuroticism + pos_affect + neg_affect + stai_trait + education_level + psqi + age + gender + Anticipation + LifeEventFamily + valence + recency + status + Intimacy + Scope + SignificanceRank'\n",
    "combined_all_variables        = 'shipley_vocab + shipley_abs + openness + conscientiousness + extraversion + agreeableness + neuroticism + pos_affect + neg_affect + stai_trait + education_level + psqi + age + gender + Anticipation + LifeEventFamily + valence + recency + status + Intimacy + Scope + data_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "bcc9ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demographics_data():\n",
    "    demographics_data = pd.read_csv('data/igtbs_demographics_complete.csv', parse_dates=True)\n",
    "    demographics_data = demographics_data[['age','gender','snapshot_id', 'shipley.vocab', 'shipley.abs', 'openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism','pos.affect','neg.affect','stai.trait','psqi','educ']]\n",
    "    demographics_data['education_level'] = demographics_data.apply(update_education_level, columnName='educ', axis=1)\n",
    "    demographics_data = demographics_data.drop(columns=['educ'])\n",
    "    demographics_data = demographics_data.rename(columns={\n",
    "        'shipley.vocab': 'shipley_vocab',\n",
    "        'shipley.abs': 'shipley_abs',\n",
    "        'pos.affect': 'pos_affect',\n",
    "        'neg.affect': 'neg_affect',\n",
    "        'stai.trait': 'stai_trait'\n",
    "    })\n",
    "    return demographics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "d8ae65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_survey_categories():\n",
    "    df_self_reported_categories = pd.read_csv('data/Life Events Categories Mapping - Self-Reported Categories.csv')\n",
    "    return df_self_reported_categories\n",
    "\n",
    "def load_survey_data_without_categories():\n",
    "    df_survey = pd.read_csv('data/Superimposed/LifeEvents_Curated_non_blinded.csv', parse_dates=True)    \n",
    "    df_survey = df_survey[['snapshot_id', 'description','UpdatedBeginDate', 'UpdatedEndDate','life_event_type', 'work_perf_impact', 'significance','valence', 'ended_or_ongoing']]\n",
    "\n",
    "    # Date manipulation\n",
    "    latest_date = max(datetime.datetime.strptime(str(x), \"%Y-%m-%d\").date() if x == x else datetime.date.min for x in df_survey['UpdatedEndDate'])\n",
    "    latest_date = max(latest_date, max(datetime.datetime.strptime(str(x), \"%Y-%m-%d\").date() if x == x else datetime.date.min for x in df_survey['UpdatedBeginDate']))\n",
    "    df_survey = df_survey.drop(df_survey[df_survey['UpdatedBeginDate'].isnull() == True].index)\n",
    "    df_survey['UpdatedEndDate'] = df_survey.apply(update_end_date, columnName='UpdatedEndDate', latestDate=latest_date, axis=1)\n",
    "    df_survey['num_of_days'] = df_survey.apply(getDays, endColumnName='UpdatedEndDate', beginColumnName='UpdatedBeginDate', axis=1)\n",
    "    df_survey['UpdatedBeginDate_time'] = df_survey.apply(convertToTime, columnName='UpdatedBeginDate', axis=1)\n",
    "    df_survey['recency'] = df_survey.apply(calculate_recency, columnName='UpdatedEndDate', axis=1)\n",
    "\n",
    "    # Update values for valence and significance\n",
    "    df_survey.replace({'valence': {np.nan: 'Neither Positive or Negative'}, 'significance': {np.nan: 'Neither Positive or Negative'}}, inplace=True)\n",
    "    df_survey['fixed_signficance'] = df_survey.apply(fix_signficance, axis = 1)\n",
    "    df_survey['fixed_valence'] = df_survey.apply(fix_valence, axis = 1)\n",
    "    df_survey = df_survey.drop(columns = ['valence', 'significance'])\n",
    "    df_survey = df_survey.rename(columns={\"fixed_signficance\": \"significance\", \"fixed_valence\": \"valence\"})\n",
    "    df_survey['valence'] = df_survey.apply(convert_valence_to_sentiment, axis=1)\n",
    "    df_survey['ended_or_ongoing'] = df_survey.apply(update_status, columnName='ended_or_ongoing', axis=1)\n",
    "\n",
    "    # Select columns we are interested in\n",
    "    df_survey = df_survey[['snapshot_id', 'description', 'UpdatedBeginDate', 'UpdatedEndDate', 'significance', 'valence', 'ended_or_ongoing', 'recency']]\n",
    "\n",
    "    # Label encoding for significance\n",
    "    le_significance = LabelEncoder()\n",
    "    le_significance.fit(df_survey['significance'].values)\n",
    "    df_survey['significance_label'] = df_survey.apply(lambda x: le_significance.transform([x['significance']])[0], axis=1)\n",
    "    df_survey = df_survey.drop(columns=['significance'])\n",
    "\n",
    "    return df_survey\n",
    "\n",
    "def load_survey_data():\n",
    "    df_survey_without_categories = load_survey_data_without_categories()\n",
    "    df_self_reported_categories = load_survey_categories()\n",
    "    df_survey = pd.merge(df_survey_without_categories, df_self_reported_categories, how=\"inner\", left_on=\"description\", right_on=\"SR_LifeEvent\")\n",
    "    df_survey = df_survey.drop(columns=['description', 'SR_LifeEvent', 'LifeEventFinal', 'LifeEventFamily2'])\n",
    "    df_survey = df_survey.rename(columns={'ended_or_ongoing':'status'})\n",
    "\n",
    "    return df_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "480efef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_social_media_categories():\n",
    "    df_social_media_categories = pd.read_csv('data/Life Events Categories Mapping - Social Media Categories-2.csv')\n",
    "    return df_social_media_categories\n",
    "\n",
    "def load_social_media_data_without_categories():\n",
    "    df_social_media_data = pd.read_csv('data/Superimposed/Facebook Data For Life Events-Combined - FB Data.csv')\n",
    "    df_social_media_data = df_social_media_data[['snapshot_id', 'created_time', 'Text', 'final_life_event_category_2', 'ended/ongoing']]\n",
    "    df_social_media_data = df_social_media_data.replace({'PostiveMove':'PositiveMove', 'Negative Move':'NegativeMove'})\n",
    "    df_social_media_data = df_social_media_data.drop(df_social_media_data[((df_social_media_data['final_life_event_category_2'].isnull() == True))].index)\n",
    "    df_social_media_data['created_date'] = df_social_media_data.apply(convertToDate, columnName='created_time', axis=1)\n",
    "    df_social_media_data['valence'] = df_social_media_data.apply(compute_sentiment, axis=1)\n",
    "    df_social_media_data = df_social_media_data.drop(columns=['created_time','Text'])\n",
    "    df_social_media_data['recency'] = df_social_media_data.apply(calculate_recency, columnName='created_date', axis=1)\n",
    "    df_social_media_data['ended/ongoing'] = df_social_media_data.apply(update_status, columnName='ended/ongoing', axis=1)\n",
    "    return df_social_media_data\n",
    "\n",
    "def load_social_media_data():\n",
    "    df_social_media_data = load_social_media_data_without_categories()\n",
    "    df_social_media_categories = load_social_media_categories()\n",
    "    df_social_media_data_with_categories = pd.merge(df_social_media_data, df_social_media_categories, how=\"inner\", left_on='final_life_event_category_2', right_on='SM_LifeEvent')\n",
    "    df_social_media_data_with_categories = df_social_media_data_with_categories.drop(columns=['final_life_event_category_2','SM_LifeEvent','LifeEventFamily2','Comments'])\n",
    "    df_social_media_data_with_categories = df_social_media_data_with_categories.rename(columns={'ended/ongoing':'status'})\n",
    "    return df_social_media_data_with_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "6108bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dailies_data():\n",
    "    df_dailies = pd.read_csv('data/Superimposed/dailies_scores.csv', low_memory=False)\n",
    "    df_dailies = df_dailies[['snapshot_id','day', 'alc_status', 'alc.quantity.d', 'anxiety.d', 'pos.affect.d', 'neg.affect.d','sleep.d', 'stress.d']]\n",
    "    df_dailies['day_time'] = df_dailies.apply(convertToTime, columnName='day', axis=1)\n",
    "    df_dailies = df_dailies.rename(columns={'alc.quantity.d': 'alc_quantity',\n",
    "    'anxiety.d': 'anxiety',\n",
    "    'pos.affect.d': 'pos_affect',\n",
    "    'neg.affect.d': 'neg_affect',\n",
    "    'sleep.d': 'sleep',\n",
    "    'stress.d': 'stress'})\n",
    "    df_dailies['sleep'] = df_dailies['sleep'] + 1\n",
    "    return df_dailies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "4e0c2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_average_by_snapshot_id(df_dailies, dependent_variable):\n",
    "    dependent_variable_by_snapshotid = {}\n",
    "\n",
    "    for i in df_dailies.iterrows():\n",
    "        snapshot_id = i[1]['snapshot_id']\n",
    "        if snapshot_id in dependent_variable_by_snapshotid:\n",
    "            dependent_variable_by_snapshotid[snapshot_id].append((i[1]['day_time'], i[1][dependent_variable]))\n",
    "        else:\n",
    "            dependent_variable_by_snapshotid[snapshot_id] = [(i[1]['day_time'], i[1][dependent_variable])]\n",
    "\n",
    "    weekly_data_by_snapshot_id = {}\n",
    "\n",
    "    for i in dependent_variable_by_snapshotid.items():\n",
    "        snapshot_id = i[0]\n",
    "        values = sorted(i[1])\n",
    "        weekly_data = {}\n",
    "        for j in values:\n",
    "            (year, week) = (j[0].isocalendar()[0], j[0].isocalendar()[1])\n",
    "            if(j[1] == j[1]):\n",
    "                if (year, week) in weekly_data:\n",
    "                    weekly_data[(year, week)].append(j[1])\n",
    "                else:\n",
    "                    weekly_data[(year, week)] = [j[1]]\n",
    "\n",
    "        for j in weekly_data.items():\n",
    "            weekly_data[j[0]] = round(mean(j[1]), 3)\n",
    "\n",
    "        weekly_data_by_snapshot_id[snapshot_id] = weekly_data\n",
    "\n",
    "    return weekly_data_by_snapshot_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "5bc49719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_for_regression_baseline(dependent_variable):\n",
    "    try:\n",
    "        merged_data = pd.read_csv(INTERMEDIATE_DATA+'baseline_regression_'+dependent_variable+'.csv')\n",
    "    except:\n",
    "        df_demographics = load_demographics_data()\n",
    "        df_dailies = load_dailies_data()\n",
    "        weekly_data_by_snapshot_id = weekly_average_by_snapshot_id(df_dailies, dependent_variable)\n",
    "\n",
    "        df_dailies = df_dailies[[dependent_variable, 'day', 'snapshot_id']]\n",
    "        merged_data = pd.merge(df_dailies, df_demographics, how=\"inner\", on=[\"snapshot_id\"])\n",
    "\n",
    "        data_per_week = []\n",
    "        merged_data[dependent_variable] = -1\n",
    "\n",
    "        for index, row in merged_data.iterrows():\n",
    "            start_date = datetime.datetime.strptime(str(row['day']), \"%Y-%m-%d\").date()\n",
    "            year, week = start_date.isocalendar()[0], start_date.isocalendar()[1]\n",
    "            if (year, week) in weekly_data_by_snapshot_id[row['snapshot_id']]:\n",
    "                v = weekly_data_by_snapshot_id[row['snapshot_id']][(year, week)]\n",
    "                row[dependent_variable] = v\n",
    "                data_per_week.append(row.copy(deep=True))\n",
    "\n",
    "        merged_data = pd.DataFrame(data_per_week, columns=merged_data.columns)\n",
    "        merged_data.reset_index(drop=True, inplace=True)\n",
    "        merged_data.to_csv(INTERMEDIATE_DATA+'baseline_regression_'+dependent_variable+'.csv', index=False)\n",
    "\n",
    "    merged_data = merged_data.drop(columns=['day', 'snapshot_id'])\n",
    "    merged_data = merged_data.dropna()\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "72a17ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_for_regression_survey_average_weekly(dependent_variable):\n",
    "    try:\n",
    "        merged_data = pd.read_csv(INTERMEDIATE_DATA+'linear_regression_survey_weekly_'+dependent_variable+'.csv')\n",
    "    except:\n",
    "        df_survey = load_survey_data()\n",
    "        df_demographics = load_demographics_data()\n",
    "        df_dailies = load_dailies_data()\n",
    "        merged_data = pd.merge(df_survey, df_demographics, how=\"inner\", on=[\"snapshot_id\"])\n",
    "\n",
    "        weekly_data_by_snapshot_id = weekly_average_by_snapshot_id(df_dailies, dependent_variable)\n",
    "        data_per_week = []\n",
    "        \n",
    "        merged_data[dependent_variable] = -1\n",
    "        merged_data['year'] = -1\n",
    "        merged_data['week'] = -1\n",
    "        for index, row in merged_data.iterrows():\n",
    "            start_date = datetime.datetime.strptime(str(row['UpdatedBeginDate']), \"%Y-%m-%d\").date()\n",
    "            end_date = datetime.datetime.strptime(str(row['UpdatedEndDate']), \"%Y-%m-%d\").date()\n",
    "            delta = timedelta(days=7)\n",
    "\n",
    "            while(start_date <= end_date):\n",
    "                year, week = start_date.isocalendar()[0], start_date.isocalendar()[1]\n",
    "                if (year, week) in weekly_data_by_snapshot_id[row['snapshot_id']]:\n",
    "                    \n",
    "                    v = weekly_data_by_snapshot_id[row['snapshot_id']][(year, week)]\n",
    "                    row[dependent_variable] = v\n",
    "                    row['year'] = year\n",
    "                    row['week'] = week\n",
    "\n",
    "                    data_per_week.append(row.copy(deep=True))\n",
    "\n",
    "                start_date+=delta\n",
    "        merged_data = pd.DataFrame(data_per_week, columns=merged_data.columns)\n",
    "        merged_data.reset_index(drop=True, inplace=True)\n",
    "        merged_data.to_csv(INTERMEDIATE_DATA+'linear_regression_survey_weekly_'+dependent_variable+'.csv', index=False)\n",
    "        \n",
    "    merged_data = merged_data.drop(columns=[ 'UpdatedBeginDate', 'UpdatedEndDate', 'snapshot_id', 'year', 'week'])\n",
    "    merged_data = merged_data.dropna()\n",
    "        \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "64a35198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_for_regression_social_media_average_weekly(dependent_variable):\n",
    "    try:\n",
    "        merged_data = pd.read_csv(INTERMEDIATE_DATA+'linear_regression_social_media_weekly_'+dependent_variable+'.csv')\n",
    "    except:\n",
    "        df_social_media = load_social_media_data()\n",
    "        df_demographics = load_demographics_data()\n",
    "        df_dailies = load_dailies_data()\n",
    "        merged_data = pd.merge(df_social_media, df_demographics, how=\"inner\", on=[\"snapshot_id\"])\n",
    "\n",
    "        weekly_data_by_snapshot_id = weekly_average_by_snapshot_id(df_dailies, dependent_variable)\n",
    "\n",
    "        merged_data[dependent_variable] = merged_data.apply(add_avg_date_based_dependent_variable, weekly_data=weekly_data_by_snapshot_id, axis=1)\n",
    "        merged_data = merged_data[merged_data[dependent_variable] != -1]\n",
    "        merged_data.to_csv(INTERMEDIATE_DATA+'linear_regression_social_media_weekly_'+dependent_variable+'.csv', index=False)\n",
    "    merged_data = merged_data.drop(columns=['snapshot_id', 'created_date'])\n",
    "    merged_data = merged_data.dropna()\n",
    "        \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "6a80e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_for_regression_combined_survey_social_media(dependent_variable):\n",
    "    df_social_media = build_df_for_regression_social_media_average_weekly(dependent_variable)\n",
    "    df_social_media['data_type'] = 'Social Media'\n",
    "    df_social_media = df_social_media.drop(columns=['SignificanceRank'])\n",
    "    \n",
    "    df_survey = build_df_for_regression_survey_average_weekly(dependent_variable)\n",
    "    df_survey['data_type'] = 'Survey'\n",
    "    df_survey = df_survey.drop(columns=['significance_label'])\n",
    "    \n",
    "    X_input = pd.concat([df_survey, df_social_media])\n",
    "    X_input = X_input.dropna()\n",
    "\n",
    "    return X_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "016d1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(X_train, dependent_variable, formula):\n",
    "    mod = smf.ols(formula=dependent_variable+'~'+formula, data=X_train)\n",
    "    res = mod.fit()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562e9205",
   "metadata": {},
   "source": [
    "## Run Regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "b458505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_control_variables(dependent_variable, split=0):\n",
    "    X_input = build_df_for_regression_baseline(dependent_variable)\n",
    "    X_input.to_csv(MODEL_DIR+'baseline_control__variables_for_'+dependent_variable+'.csv', index=False)\n",
    "\n",
    "    if split == 0:\n",
    "        return regression(X_input, dependent_variable, control_variables)\n",
    "    else:\n",
    "        X_train, X_test = train_test_split(X_input, test_size=split, random_state=85)\n",
    "        model = regression(X_train, dependent_variable, control_variables)\n",
    "        return X_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "49266b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def life_event_variables_all_data(dependent_variable, split=0):\n",
    "    X_input = build_df_for_regression_combined_survey_social_media(dependent_variable)\n",
    "    X_input.to_csv(MODEL_DIR+'life_event_variables_all_data_for_'+dependent_variable+'.csv', index=False)\n",
    "    \n",
    "    if split == 0:\n",
    "        return regression(X_input, dependent_variable, combined_life_event_variables)\n",
    "    else:\n",
    "        X_train, X_test = train_test_split(X_input, test_size=split, random_state=85)\n",
    "        model = regression(X_train, dependent_variable, combined_life_event_variables)\n",
    "        return X_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "d605240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def life_event_variables_sr(dependent_variable, split=0):\n",
    "    X_input = build_df_for_regression_survey_average_weekly(dependent_variable)\n",
    "    X_input.to_csv(MODEL_DIR+'life_event_variables_sr_for_'+dependent_variable+'.csv', index=False)\n",
    "\n",
    "    if split == 0:\n",
    "        return regression(X_input, dependent_variable, sr_life_event_variables)\n",
    "    else:\n",
    "        X_train, X_test = train_test_split(X_input, test_size=split, random_state=85)\n",
    "        model = regression(X_train, dependent_variable, sr_life_event_variables)\n",
    "        return X_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "4db194f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def life_event_variables_sm(dependent_variable, split=0):\n",
    "    X_input = build_df_for_regression_social_media_average_weekly(dependent_variable)\n",
    "    X_input.to_csv(MODEL_DIR+'life_event_variables_sm_for_'+dependent_variable+'.csv', index=False)\n",
    "\n",
    "    if split == 0:\n",
    "        return regression(X_input, dependent_variable, sm_life_event_variables)\n",
    "    else:\n",
    "        X_train, X_test = train_test_split(X_input, test_size=split, random_state=85)\n",
    "        model = regression(X_train, dependent_variable, sm_life_event_variables)\n",
    "        return X_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "5f8d3a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_variables_all_data(dependent_variable, split=0):\n",
    "    X_input = build_df_for_regression_combined_survey_social_media(dependent_variable)\n",
    "    X_input.to_csv(MODEL_DIR+'all_variables_all_data_for_'+dependent_variable+'.csv', index=False)\n",
    "\n",
    "    if split == 0:\n",
    "        return regression(X_input, dependent_variable, combined_all_variables)\n",
    "    else:\n",
    "        X_train, X_test = train_test_split(X_input, test_size=split, random_state=85)\n",
    "        model = regression(X_train, dependent_variable, combined_all_variables)\n",
    "        return X_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "6f9171c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_variables_sr(dependent_variable, split=0):\n",
    "    X_input = build_df_for_regression_survey_average_weekly(dependent_variable)\n",
    "    X_input.to_csv(MODEL_DIR+'all_variables_sr_for_'+dependent_variable+'.csv', index=False)\n",
    "\n",
    "    if split == 0:\n",
    "        return regression(X_input, dependent_variable, sr_all_variables)\n",
    "    else:\n",
    "        X_train, X_test = train_test_split(X_input, test_size=split, random_state=85)\n",
    "        model = regression(X_train, dependent_variable, sr_all_variables)\n",
    "        return X_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "bdca8bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_variables_sm(dependent_variable, split=0):\n",
    "    X_input = build_df_for_regression_social_media_average_weekly(dependent_variable)\n",
    "    X_input.to_csv(MODEL_DIR+'all_variables_sm_for_'+dependent_variable+'.csv', index=False)\n",
    "\n",
    "    if split == 0:\n",
    "        return regression(X_input, dependent_variable, sm_all_variables)\n",
    "    else:\n",
    "        X_train, X_test = train_test_split(X_input, test_size=split, random_state=85)\n",
    "        model = regression(X_train, dependent_variable, sm_all_variables)\n",
    "        return X_test, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a61fb9b",
   "metadata": {},
   "source": [
    "# Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "6f540bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline_fn(fn, dependent_variable):\n",
    "    model = fn(dependent_variable)\n",
    "    dataset, train_model = fn(dependent_variable, 0.20)\n",
    "\n",
    "    f_obs = train_model.predict(dataset).values\n",
    "    f_exp = dataset[dependent_variable].values\n",
    "\n",
    "    pearson = scipy.stats.pearsonr(f_obs, f_exp)\n",
    "    return model, [round(model.rsquared_adj, 3), round(pearson[0], 3), \"-\", \"-\"]\n",
    "\n",
    "def run_regression_fn(baseline, fn, dependent_variable):\n",
    "    model = fn(dependent_variable)\n",
    "\n",
    "    dataset, train_model = fn(dependent_variable, 0.20)\n",
    "\n",
    "    f_obs = train_model.predict(dataset).values\n",
    "    f_exp = dataset[dependent_variable].values\n",
    "\n",
    "    pearson = scipy.stats.pearsonr(f_obs, f_exp)\n",
    "    variance = anova_lm(baseline, model)\n",
    "    return [round(model.rsquared_adj, 3), round(pearson[0], 3), round(variance.F.values[-1], 3), round(variance['Pr(>F)'].values[-1], 3)]\n",
    "\n",
    "\n",
    "def run_all_regression(dependent_variable):\n",
    "    data = []\n",
    "    baseline_model, results = run_baseline_fn(baseline_control_variables, dependent_variable)\n",
    "    data.append([\"Dailies\", \"Control\"] + results)\n",
    "    \n",
    "    data.append([\"Social Media + Survey\", \"Life Event\"] + run_regression_fn(baseline_model, life_event_variables_all_data, dependent_variable))\n",
    "    data.append([\"Social Media + Survey\", \"Control + Life Event\"] + run_regression_fn(baseline_model, all_variables_all_data, dependent_variable))\n",
    "    \n",
    "    data.append([\"Social Media\", \"Life Event\"] + run_regression_fn(baseline_model, life_event_variables_sm, dependent_variable))\n",
    "    data.append([\"Social Media\", \"Control + Life Event\"] + run_regression_fn(baseline_model, all_variables_sm, dependent_variable))\n",
    "\n",
    "    data.append([\"Survey\", \"Life Event\"] + run_regression_fn(baseline_model, life_event_variables_sr, dependent_variable))\n",
    "    data.append([\"Survey\", \"Control + Life Event\"] + run_regression_fn(baseline_model, all_variables_sr, dependent_variable))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509fe27f",
   "metadata": {},
   "source": [
    "### STRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "3e65b24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Variables</th>\n",
       "      <th>Adjusted R-square</th>\n",
       "      <th>Pearson Correlation</th>\n",
       "      <th>F-statistic</th>\n",
       "      <th>P-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dailies</td>\n",
       "      <td>Control</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Social Media + Survey</td>\n",
       "      <td>Life Event</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.733</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Social Media + Survey</td>\n",
       "      <td>Control + Life Event</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.851</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Social Media</td>\n",
       "      <td>Life Event</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.718</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Social Media</td>\n",
       "      <td>Control + Life Event</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.811</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Survey</td>\n",
       "      <td>Life Event</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Survey</td>\n",
       "      <td>Control + Life Event</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Dataset             Variables  Adjusted R-square  \\\n",
       "0                Dailies               Control              0.152   \n",
       "1  Social Media + Survey            Life Event              0.039   \n",
       "2  Social Media + Survey  Control + Life Event              0.165   \n",
       "3           Social Media            Life Event              0.057   \n",
       "4           Social Media  Control + Life Event              0.163   \n",
       "5                 Survey            Life Event              0.030   \n",
       "6                 Survey  Control + Life Event              0.179   \n",
       "\n",
       "   Pearson Correlation F-statistic P-value  \n",
       "0                0.393           -       -  \n",
       "1                0.152       0.733       1  \n",
       "2                0.432       0.851       1  \n",
       "3                0.252       0.718       1  \n",
       "4                0.380       0.811       1  \n",
       "5                0.207        0.77       1  \n",
       "6                0.308       0.916   0.982  "
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = run_all_regression('stress')\n",
    "regression_results = pd.DataFrame(d, columns=[\"Dataset\", \"Variables\", \"Adjusted R-square\", \"Pearson Correlation\", \"F-statistic\", \"P-value\"])\n",
    "regression_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0672858a",
   "metadata": {},
   "source": [
    "### SLEEP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "722290ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Variables</th>\n",
       "      <th>Adjusted R-square</th>\n",
       "      <th>Pearson Correlation</th>\n",
       "      <th>F-statistic</th>\n",
       "      <th>P-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dailies</td>\n",
       "      <td>Control</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Social Media + Survey</td>\n",
       "      <td>Life Event</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Social Media + Survey</td>\n",
       "      <td>Control + Life Event</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Social Media</td>\n",
       "      <td>Life Event</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.597</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Social Media</td>\n",
       "      <td>Control + Life Event</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Survey</td>\n",
       "      <td>Life Event</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.781</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Survey</td>\n",
       "      <td>Control + Life Event</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Dataset             Variables  Adjusted R-square  \\\n",
       "0                Dailies               Control              0.053   \n",
       "1  Social Media + Survey            Life Event              0.024   \n",
       "2  Social Media + Survey  Control + Life Event              0.093   \n",
       "3           Social Media            Life Event              0.020   \n",
       "4           Social Media  Control + Life Event              0.124   \n",
       "5                 Survey            Life Event              0.041   \n",
       "6                 Survey  Control + Life Event              0.116   \n",
       "\n",
       "   Pearson Correlation F-statistic P-value  \n",
       "0                0.225           -       -  \n",
       "1                0.140       0.695       1  \n",
       "2                0.314       0.751       1  \n",
       "3                0.002       0.597       1  \n",
       "4                0.245        0.67       1  \n",
       "5                0.165       0.781       1  \n",
       "6                0.311        0.85       1  "
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = run_all_regression('sleep')\n",
    "regression_results = pd.DataFrame(d, columns=[\"Dataset\", \"Variables\", \"Adjusted R-square\", \"Pearson Correlation\", \"F-statistic\", \"P-value\"])\n",
    "regression_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d29f2",
   "metadata": {},
   "source": [
    "### ANXIETY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "4cf45694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Variables</th>\n",
       "      <th>Adjusted R-square</th>\n",
       "      <th>Pearson Correlation</th>\n",
       "      <th>F-statistic</th>\n",
       "      <th>P-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dailies</td>\n",
       "      <td>Control</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Social Media + Survey</td>\n",
       "      <td>Life Event</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.734</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Social Media + Survey</td>\n",
       "      <td>Control + Life Event</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Social Media</td>\n",
       "      <td>Life Event</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.819</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Social Media</td>\n",
       "      <td>Control + Life Event</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Survey</td>\n",
       "      <td>Life Event</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Survey</td>\n",
       "      <td>Control + Life Event</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Dataset             Variables  Adjusted R-square  \\\n",
       "0                Dailies               Control              0.183   \n",
       "1  Social Media + Survey            Life Event              0.062   \n",
       "2  Social Media + Survey  Control + Life Event              0.236   \n",
       "3           Social Media            Life Event              0.047   \n",
       "4           Social Media  Control + Life Event              0.185   \n",
       "5                 Survey            Life Event              0.049   \n",
       "6                 Survey  Control + Life Event              0.259   \n",
       "\n",
       "   Pearson Correlation F-statistic P-value  \n",
       "0                0.436           -       -  \n",
       "1                0.209       0.734       1  \n",
       "2                0.434       0.912   0.997  \n",
       "3                0.235       0.819       1  \n",
       "4                0.456       0.961   0.761  \n",
       "5                0.162        0.72       1  \n",
       "6                0.411       0.932   0.954  "
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = run_all_regression('anxiety')\n",
    "regression_results = pd.DataFrame(d, columns=[\"Dataset\", \"Variables\", \"Adjusted R-square\", \"Pearson Correlation\", \"F-statistic\", \"P-value\"])\n",
    "regression_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
